{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Translator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOoUSaUrj7E/CMVZaWKWquq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RVS97/English-French-Translator/blob/master/Translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7eEJTIk6Jq6",
        "colab_type": "text"
      },
      "source": [
        "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyYpTWG64nPR",
        "colab_type": "text"
      },
      "source": [
        "#Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-ObM2zO2Wf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "from google.colab import drive\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYZmjgEh_NtG",
        "colab_type": "text"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ssgwejMALZP",
        "colab_type": "text"
      },
      "source": [
        "##Loading Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmL6b5bYANhM",
        "colab_type": "code",
        "outputId": "4fa91c92-88d9-486b-fe81-09cf248406ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPWOj3QzAfQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/drive/path_to_data/europarl-v7.fr-en.en\",\n",
        "          mode='r',\n",
        "          encoding=\"utf-8\") as f:\n",
        "    europarl_en = f.read()\n",
        "with open(\"/content/drive/path_to_data/europarl-v7.fr-en.fr\",\n",
        "          mode='r',\n",
        "          encoding=\"utf-8\") as f:\n",
        "    europarl_fr = f.read()\n",
        "with open(\"/content/drive/path_to_data/nonbreaking_prefix.en\",\n",
        "          mode='r',\n",
        "          encoding=\"utf-8\") as f:\n",
        "    non_breaking_prefix_en = f.read()\n",
        "with open(\"/content/drive/path_to_data/nonbreaking_prefix.fr\",\n",
        "          mode='r',\n",
        "          encoding=\"utf-8\") as f:\n",
        "    non_breaking_prefix_fr = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pno6XO1t_RXH",
        "colab_type": "text"
      },
      "source": [
        "##Cleaning Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c2Xd93VBTMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en = [' '+pref+'.' for pref in non_breaking_prefix_en]\n",
        "non_breaking_prefix_fr = non_breaking_prefix_fr.split(\"\\n\")\n",
        "non_breaking_prefix_fr = [' '+pref+'.' for pref in non_breaking_prefix_fr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPxoLK6DBzfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_en = europarl_en\n",
        "for prefix in non_breaking_prefix_en:\n",
        "  corpus_en = corpus_en.replace(prefix, prefix+\"###\")\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", corpus_en)\n",
        "corpus_en = re.sub(r\"\\.###\",'',corpus_en)\n",
        "corpus_en = re.sub(r\"  +\",' ',corpus_en)\n",
        "corpus_en = corpus_en.split(\"\\n\")\n",
        "\n",
        "corpus_fr = europarl_fr\n",
        "for prefix in non_breaking_prefix_fr:\n",
        "  corpus_fr = corpus_fr.replace(prefix, prefix+\"###\")\n",
        "corpus_fr = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", corpus_fr)\n",
        "corpus_fr = re.sub(r\"\\.###\",'',corpus_fr)\n",
        "corpus_fr = re.sub(r\"  +\",' ',corpus_fr)\n",
        "corpus_fr = corpus_fr.split(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP26BqvGDXuN",
        "colab_type": "text"
      },
      "source": [
        "##Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NijckjHDDaVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_en, target_vocab_size=2**13)\n",
        "tokenizer_fr = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_fr, target_vocab_size=2**13)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAYTu655DvYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
        "VOCAB_SIZE_FR = tokenizer_fr.vocab_size + 2\n",
        "\n",
        "# VOCAB_SIZE_EN -1 OR -2 used as start and end sentence tokens\n",
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1] for sentence in corpus_en]\n",
        "outputs = [[VOCAB_SIZE_FR-2] + tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR-1] for sentence in corpus_fr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIQnMKzbFO8G",
        "colab_type": "text"
      },
      "source": [
        "##Remove too long sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAJ_1MpvFSlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs) if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):  # reverse order to avoid idx change\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs) if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):  # reverse order to avoid idx change\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU11XFDfGQiW",
        "colab_type": "text"
      },
      "source": [
        "##Inputs/Outputs creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELmF0rPsGUxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,value=0,padding=\"post\",maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,value=0,padding=\"post\",maxlen=MAX_LENGTH)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qtrDdI_Hhdr",
        "colab_type": "text"
      },
      "source": [
        "#Model Build"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84OPiS0oHkR5",
        "colab_type": "text"
      },
      "source": [
        "##Embedding and Positional Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G9C3ucmJ86I",
        "colab_type": "text"
      },
      "source": [
        "Positional encoding formulae:\n",
        "\n",
        "$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n",
        "\n",
        "$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw4FVDM4Hw0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "\n",
        "  def get_angles(self,pos,i,d_model): # pos: (seq_length, 1) i: (1, d_model)\n",
        "    angles = 1 / np.power(10000., (2*(i//2))/np.float32(d_model))\n",
        "    return pos * angles # (seq_length, d_model)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    seq_length = inputs.shape.as_list()[-2]\n",
        "    d_model = inputs.shape.as_list()[-1]\n",
        "    # call get_angles formatting by expanding dims\n",
        "    angles = self.get_angles(np.arange(seq_length)[:,np.newaxis],\n",
        "                             np.arange(d_model)[np.newaxis, :],\n",
        "                             d_model)\n",
        "    angles[:, 0::2] = np.sin(angles[:, 0::2]) #even\n",
        "    angles[:, 1::2] = np.cos(angles[:, 1::2]) #odd\n",
        "    pos_encoding = angles[np.newaxis, ...] #expand dims of angles\n",
        "    return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlybSv0SLZzW",
        "colab_type": "text"
      },
      "source": [
        "##Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA14PPscLboU",
        "colab_type": "text"
      },
      "source": [
        "###Attention computation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VBuW6lESLDX",
        "colab_type": "text"
      },
      "source": [
        "$Attention(Q, K, V ) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM9L8soOLhmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "  product = tf.matmul(queries, keys, transpose_b=True)\n",
        "\n",
        "  keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "  scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_product += (mask * -1e9) # add -inf for softmax -> 0\n",
        "\n",
        "  attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
        "\n",
        "  return attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_-bomZoND1D",
        "colab_type": "text"
      },
      "source": [
        "###Multi-head attention sublayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCOUk5vxNHfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "\n",
        "  def __init__(self, nb_proj):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.nb_proj = nb_proj\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "    assert self.d_model % self.nb_proj == 0\n",
        "\n",
        "    self.d_proj = self.d_model // self.nb_proj\n",
        "\n",
        "    self.query_lin = layers.Dense(units=self.d_model)\n",
        "    self.key_lin = layers.Dense(units=self.d_model)\n",
        "    self.value_lin = layers.Dense(units=self.d_model)\n",
        "\n",
        "    self.final_lin = layers.Dense(units=self.d_model)\n",
        "\n",
        "  def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
        "    shape = (batch_size, -1, self.nb_proj, self.d_proj)\n",
        "    splitted_inputs = tf.reshape(inputs, shape=shape) #(batch_size, seq_length, nb_proj, d_proj)\n",
        "    return tf.transpose(splitted_inputs, perm=[0,2,1,3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
        "\n",
        "  def call(self, queries, keys, values, mask):\n",
        "    batch_size = tf.shape(queries)[0]\n",
        "\n",
        "    queries = self.query_lin(queries)\n",
        "    keys = self.key_lin(keys)\n",
        "    values = self.value_lin(values)\n",
        "\n",
        "    queries = self.split_proj(queries, batch_size)\n",
        "    keys = self.split_proj(keys, batch_size)\n",
        "    values = self.split_proj(values, batch_size)\n",
        "\n",
        "    attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "\n",
        "    attention = tf.transpose(attention, perm=[0,2,1,3])\n",
        "\n",
        "    concat_attention = tf.reshape(attention, shape=(batch_size, -1, self.d_model))\n",
        "\n",
        "    outputs = self.final_lin(concat_attention)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SChF0wocoo9n",
        "colab_type": "text"
      },
      "source": [
        "##Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGZh9SfCoqc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "\n",
        "  def __init__(self, FFN_units, nb_proj, dropout):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.FFN_units = FFN_units\n",
        "    self.nb_proj = nb_proj\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "\n",
        "    self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "    self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "    self.dense_2 = layers.Dense(units=self.d_model)\n",
        "    self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "    self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "  def call(self, inputs, mask, training):\n",
        "    attention = self.multi_head_attention(inputs, inputs, inputs, mask)\n",
        "    attention = self.dropout_1(attention, training=training)\n",
        "    attention = self.norm_1(attention + inputs)\n",
        "\n",
        "    outputs = self.dense_1(attention)\n",
        "    outputs = self.dense_2(outputs)\n",
        "    outputs = self.dropout_2(outputs, training=training)\n",
        "    outputs = self.norm_2(outputs + attention)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RBFT-AvrhsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "\n",
        "  def __init__(self, nb_layers, FFN_units, nb_proj, dropout, vocab_size, d_model, name=\"encoder\"):\n",
        "    super(Encoder, self).__init__(name=name)\n",
        "    self.nb_layers = nb_layers\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding()\n",
        "    self.dropout = layers.Dropout(rate=dropout)\n",
        "    self.enc_layers = [EncoderLayer(FFN_units, nb_proj, dropout)\n",
        "                      for _ in range(nb_layers)]\n",
        "\n",
        "  def call(self, inputs, mask, training):\n",
        "    outputs = self.embedding(inputs)\n",
        "    outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    outputs = self.pos_encoding(outputs)\n",
        "    outputs = self.dropout(outputs, training)\n",
        "\n",
        "    for i in range(self.nb_layers):\n",
        "      outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "    return outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoDP1mFIcW3w",
        "colab_type": "text"
      },
      "source": [
        "##Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qdYX1ZFclmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "\n",
        "  def __init__(self, FFN_units, nb_proj, dropout):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.FFN_units = FFN_units\n",
        "    self.nb_proj = nb_proj\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "\n",
        "    self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "    self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "    self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "    self.dense_2 = layers.Dense(units=self.d_model)\n",
        "    self.dropout_3 = layers.Dropout(rate=self.dropout)\n",
        "    self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "  def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "    attention = self.multi_head_attention_1(inputs, inputs, inputs, mask_1)\n",
        "    attention = self.dropout_1(attention, training)\n",
        "    attention = self.norm_1(attention + inputs)\n",
        "\n",
        "    attention_2 = self.multi_head_attention_2(attention, enc_outputs, enc_outputs, mask_2)\n",
        "    attention_2 = self.dropout_2(attention_2, training)\n",
        "    attention_2 = self.norm_2(attention_2 + attention)\n",
        "\n",
        "    outputs = self.dense_1(attention_2)\n",
        "    outputs = self.dense_2(outputs)\n",
        "    outputs = self.dropout_3(outputs, training)\n",
        "    outputs = self.norm_3(outputs + attention_2)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tMudiBKfRpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "\n",
        "  def __init__(self, nb_layers, FFN_units, nb_proj, dropout, vocab_size, d_model, name=\"decoder\"):\n",
        "    super(Decoder, self).__init__(name=name)\n",
        "    self.d_model = d_model\n",
        "    self.nb_layers = nb_layers\n",
        "\n",
        "    self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding()\n",
        "    self.dropout = layers.Dropout(rate=dropout)\n",
        "\n",
        "    self.dec_layers = [DecoderLayer(FFN_units, nb_proj, dropout)\n",
        "                      for _ in range(nb_layers)]\n",
        "  def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "    outputs = self.embedding(inputs)\n",
        "    outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    outputs = self.pos_encoding(outputs)\n",
        "    outputs = self.dropout(outputs, training)\n",
        "\n",
        "    for i in range(self.nb_layers):\n",
        "      outputs = self.dec_layers[i](outputs, enc_outputs, mask_1, mask_2, training)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTK3_SfnhKW6",
        "colab_type": "text"
      },
      "source": [
        "##Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADVs4AywhOic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, vocab_size_enc, vocab_size_dec, d_model, nb_layers, FFN_units, nb_proj, dropout, name=\"transformer\"):\n",
        "    super(Transformer, self).__init__(name=name)\n",
        "\n",
        "    self.encoder = Encoder(nb_layers, FFN_units, nb_proj, dropout, vocab_size_enc, d_model)\n",
        "    self.decoder = Decoder(nb_layers, FFN_units, nb_proj, dropout, vocab_size_dec, d_model)\n",
        "    self.last_linear = layers.Dense(units=vocab_size_dec)\n",
        "\n",
        "  def create_padding_mask(self, seq): # seq: (batch_size, seq_length)\n",
        "    # padding for 0s in sentence\n",
        "    mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  def create_look_ahead_mask(self, seq):\n",
        "    # padding to avoid knowing future words (triangular matrix (upper))\n",
        "    seq_len = tf.shape(seq)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    return look_ahead_mask\n",
        "\n",
        "  def call(self, enc_inputs, dec_inputs, training):\n",
        "    enc_mask = self.create_padding_mask(enc_inputs)\n",
        "    dec_mask_1 = tf.maximum(self.create_padding_mask(dec_inputs), self.create_look_ahead_mask(dec_inputs))\n",
        "    dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "\n",
        "    enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "    dec_outputs = self.decoder(dec_inputs, enc_outputs, dec_mask_1, dec_mask_2, training)\n",
        "\n",
        "    outputs = self.last_linear(dec_outputs)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtQP60zlnm-X",
        "colab_type": "text"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xyu8XoUynofY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hyper-parameters\n",
        "D_MODEL = 128 # 512\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN, \n",
        "                          vocab_size_dec=VOCAB_SIZE_FR, \n",
        "                          d_model=D_MODEL, \n",
        "                          nb_layers=NB_LAYERS, \n",
        "                          FFN_units=FFN_UNITS, \n",
        "                          nb_proj=NB_PROJ, \n",
        "                          dropout=DROPOUT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5AVZoqnAstL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
        "\n",
        "def loss_function(target, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "  loss_ = loss_object(target, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *=mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpW3tMmlBW5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = tf.cast(d_model, tf.float32)\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ8iR81jCcFk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "534ca7cb-feb9-4abc-8319-a01e76ee7428"
      },
      "source": [
        "checkpoint_path = \"./drive/path_to_ckpt/checkpoint/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print(\"Latest checkpoint restored!!!\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latest checkpoint restored!!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AER0RLFtFtlZ",
        "colab_type": "code",
        "outputId": "7d8061aa-b4fb-4ec7-e1fe-b4385822d1c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "  print(\"Start of epoch {}\".format(epoch+1))\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "    dec_inputs = targets[:, :-1]\n",
        "    dec_outputs_real = targets[:, 1:]\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "      loss = loss_function(dec_outputs_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(dec_outputs_real, predictions)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "          epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "      \n",
        "  ckpt_save_path = ckpt_manager.save()\n",
        "  print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1, ckpt_save_path))\n",
        "  print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 6.4870 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 6.2294 Accuracy 0.0055\n",
            "Epoch 1 Batch 100 Loss 6.1525 Accuracy 0.0284\n",
            "Epoch 1 Batch 150 Loss 6.0901 Accuracy 0.0364\n",
            "Epoch 1 Batch 200 Loss 6.0121 Accuracy 0.0405\n",
            "Epoch 1 Batch 250 Loss 5.9159 Accuracy 0.0429\n",
            "Epoch 1 Batch 300 Loss 5.7981 Accuracy 0.0472\n",
            "Epoch 1 Batch 350 Loss 5.6720 Accuracy 0.0536\n",
            "Epoch 1 Batch 400 Loss 5.5425 Accuracy 0.0587\n",
            "Epoch 1 Batch 450 Loss 5.4288 Accuracy 0.0636\n",
            "Epoch 1 Batch 500 Loss 5.3162 Accuracy 0.0685\n",
            "Epoch 1 Batch 550 Loss 5.2111 Accuracy 0.0738\n",
            "Epoch 1 Batch 600 Loss 5.1121 Accuracy 0.0792\n",
            "Epoch 1 Batch 650 Loss 5.0175 Accuracy 0.0845\n",
            "Epoch 1 Batch 700 Loss 4.9288 Accuracy 0.0897\n",
            "Epoch 1 Batch 750 Loss 4.8454 Accuracy 0.0948\n",
            "Epoch 1 Batch 800 Loss 4.7650 Accuracy 0.1000\n",
            "Epoch 1 Batch 850 Loss 4.6886 Accuracy 0.1049\n",
            "Epoch 1 Batch 900 Loss 4.6143 Accuracy 0.1097\n",
            "Epoch 1 Batch 950 Loss 4.5437 Accuracy 0.1144\n",
            "Epoch 1 Batch 1000 Loss 4.4800 Accuracy 0.1188\n",
            "Epoch 1 Batch 1050 Loss 4.4192 Accuracy 0.1229\n",
            "Epoch 1 Batch 1100 Loss 4.3630 Accuracy 0.1269\n",
            "Epoch 1 Batch 1150 Loss 4.3113 Accuracy 0.1305\n",
            "Epoch 1 Batch 1200 Loss 4.2599 Accuracy 0.1340\n",
            "Epoch 1 Batch 1250 Loss 4.2108 Accuracy 0.1372\n",
            "Epoch 1 Batch 1300 Loss 4.1643 Accuracy 0.1403\n",
            "Epoch 1 Batch 1350 Loss 4.1206 Accuracy 0.1435\n",
            "Epoch 1 Batch 1400 Loss 4.0794 Accuracy 0.1466\n",
            "Epoch 1 Batch 1450 Loss 4.0390 Accuracy 0.1497\n",
            "Epoch 1 Batch 1500 Loss 4.0007 Accuracy 0.1527\n",
            "Epoch 1 Batch 1550 Loss 3.9637 Accuracy 0.1557\n",
            "Epoch 1 Batch 1600 Loss 3.9278 Accuracy 0.1586\n",
            "Epoch 1 Batch 1650 Loss 3.8932 Accuracy 0.1614\n",
            "Epoch 1 Batch 1700 Loss 3.8618 Accuracy 0.1641\n",
            "Epoch 1 Batch 1750 Loss 3.8296 Accuracy 0.1668\n",
            "Epoch 1 Batch 1800 Loss 3.7980 Accuracy 0.1694\n",
            "Epoch 1 Batch 1850 Loss 3.7682 Accuracy 0.1720\n",
            "Epoch 1 Batch 1900 Loss 3.7390 Accuracy 0.1744\n",
            "Epoch 1 Batch 1950 Loss 3.7111 Accuracy 0.1768\n",
            "Epoch 1 Batch 2000 Loss 3.6832 Accuracy 0.1791\n",
            "Epoch 1 Batch 2050 Loss 3.6572 Accuracy 0.1812\n",
            "Epoch 1 Batch 2100 Loss 3.6314 Accuracy 0.1831\n",
            "Epoch 1 Batch 2150 Loss 3.6049 Accuracy 0.1850\n",
            "Epoch 1 Batch 2200 Loss 3.5787 Accuracy 0.1869\n",
            "Epoch 1 Batch 2250 Loss 3.5529 Accuracy 0.1887\n",
            "Epoch 1 Batch 2300 Loss 3.5278 Accuracy 0.1906\n",
            "Epoch 1 Batch 2350 Loss 3.5042 Accuracy 0.1924\n",
            "Epoch 1 Batch 2400 Loss 3.4805 Accuracy 0.1943\n",
            "Epoch 1 Batch 2450 Loss 3.4569 Accuracy 0.1962\n",
            "Epoch 1 Batch 2500 Loss 3.4346 Accuracy 0.1982\n",
            "Epoch 1 Batch 2550 Loss 3.4120 Accuracy 0.2001\n",
            "Epoch 1 Batch 2600 Loss 3.3897 Accuracy 0.2020\n",
            "Epoch 1 Batch 2650 Loss 3.3673 Accuracy 0.2038\n",
            "Epoch 1 Batch 2700 Loss 3.3462 Accuracy 0.2057\n",
            "Epoch 1 Batch 2750 Loss 3.3257 Accuracy 0.2076\n",
            "Epoch 1 Batch 2800 Loss 3.3049 Accuracy 0.2094\n",
            "Epoch 1 Batch 2850 Loss 3.2851 Accuracy 0.2112\n",
            "Epoch 1 Batch 2900 Loss 3.2653 Accuracy 0.2131\n",
            "Epoch 1 Batch 2950 Loss 3.2457 Accuracy 0.2148\n",
            "Epoch 1 Batch 3000 Loss 3.2270 Accuracy 0.2166\n",
            "Epoch 1 Batch 3050 Loss 3.2078 Accuracy 0.2183\n",
            "Epoch 1 Batch 3100 Loss 3.1894 Accuracy 0.2201\n",
            "Epoch 1 Batch 3150 Loss 3.1708 Accuracy 0.2219\n",
            "Epoch 1 Batch 3200 Loss 3.1527 Accuracy 0.2236\n",
            "Epoch 1 Batch 3250 Loss 3.1344 Accuracy 0.2254\n",
            "Epoch 1 Batch 3300 Loss 3.1169 Accuracy 0.2273\n",
            "Epoch 1 Batch 3350 Loss 3.0993 Accuracy 0.2290\n",
            "Epoch 1 Batch 3400 Loss 3.0826 Accuracy 0.2307\n",
            "Epoch 1 Batch 3450 Loss 3.0662 Accuracy 0.2325\n",
            "Epoch 1 Batch 3500 Loss 3.0500 Accuracy 0.2342\n",
            "Epoch 1 Batch 3550 Loss 3.0341 Accuracy 0.2359\n",
            "Epoch 1 Batch 3600 Loss 3.0181 Accuracy 0.2377\n",
            "Epoch 1 Batch 3650 Loss 3.0028 Accuracy 0.2394\n",
            "Epoch 1 Batch 3700 Loss 2.9873 Accuracy 0.2411\n",
            "Epoch 1 Batch 3750 Loss 2.9723 Accuracy 0.2428\n",
            "Epoch 1 Batch 3800 Loss 2.9575 Accuracy 0.2445\n",
            "Epoch 1 Batch 3850 Loss 2.9429 Accuracy 0.2462\n",
            "Epoch 1 Batch 3900 Loss 2.9283 Accuracy 0.2478\n",
            "Epoch 1 Batch 3950 Loss 2.9139 Accuracy 0.2494\n",
            "Epoch 1 Batch 4000 Loss 2.8996 Accuracy 0.2511\n",
            "Epoch 1 Batch 4050 Loss 2.8857 Accuracy 0.2527\n",
            "Epoch 1 Batch 4100 Loss 2.8721 Accuracy 0.2543\n",
            "Epoch 1 Batch 4150 Loss 2.8595 Accuracy 0.2557\n",
            "Epoch 1 Batch 4200 Loss 2.8470 Accuracy 0.2571\n",
            "Epoch 1 Batch 4250 Loss 2.8355 Accuracy 0.2583\n",
            "Epoch 1 Batch 4300 Loss 2.8243 Accuracy 0.2596\n",
            "Epoch 1 Batch 4350 Loss 2.8133 Accuracy 0.2608\n",
            "Epoch 1 Batch 4400 Loss 2.8029 Accuracy 0.2619\n",
            "Epoch 1 Batch 4450 Loss 2.7925 Accuracy 0.2631\n",
            "Epoch 1 Batch 4500 Loss 2.7824 Accuracy 0.2642\n",
            "Epoch 1 Batch 4550 Loss 2.7725 Accuracy 0.2653\n",
            "Epoch 1 Batch 4600 Loss 2.7627 Accuracy 0.2663\n",
            "Epoch 1 Batch 4650 Loss 2.7529 Accuracy 0.2674\n",
            "Epoch 1 Batch 4700 Loss 2.7435 Accuracy 0.2685\n",
            "Epoch 1 Batch 4750 Loss 2.7336 Accuracy 0.2695\n",
            "Epoch 1 Batch 4800 Loss 2.7245 Accuracy 0.2706\n",
            "Epoch 1 Batch 4850 Loss 2.7146 Accuracy 0.2716\n",
            "Epoch 1 Batch 4900 Loss 2.7055 Accuracy 0.2726\n",
            "Epoch 1 Batch 4950 Loss 2.6966 Accuracy 0.2736\n",
            "Epoch 1 Batch 5000 Loss 2.6876 Accuracy 0.2746\n",
            "Epoch 1 Batch 5050 Loss 2.6790 Accuracy 0.2756\n",
            "Epoch 1 Batch 5100 Loss 2.6702 Accuracy 0.2765\n",
            "Epoch 1 Batch 5150 Loss 2.6616 Accuracy 0.2775\n",
            "Epoch 1 Batch 5200 Loss 2.6529 Accuracy 0.2784\n",
            "Epoch 1 Batch 5250 Loss 2.6441 Accuracy 0.2793\n",
            "Epoch 1 Batch 5300 Loss 2.6352 Accuracy 0.2801\n",
            "Epoch 1 Batch 5350 Loss 2.6270 Accuracy 0.2810\n",
            "Epoch 1 Batch 5400 Loss 2.6185 Accuracy 0.2818\n",
            "Epoch 1 Batch 5450 Loss 2.6100 Accuracy 0.2826\n",
            "Epoch 1 Batch 5500 Loss 2.6019 Accuracy 0.2834\n",
            "Epoch 1 Batch 5550 Loss 2.5936 Accuracy 0.2842\n",
            "Epoch 1 Batch 5600 Loss 2.5855 Accuracy 0.2850\n",
            "Epoch 1 Batch 5650 Loss 2.5774 Accuracy 0.2858\n",
            "Epoch 1 Batch 5700 Loss 2.5692 Accuracy 0.2867\n",
            "Saving checkpoint for epoch 1 at ./drive/My Drive/Colab Notebooks/ModernNLP/Transformer/checkpoint/ckpt-1\n",
            "Time taken for 1 epoch: 1458.2919342517853 secs\n",
            "\n",
            "Start of epoch 2\n",
            "Epoch 2 Batch 0 Loss 1.6295 Accuracy 0.4005\n",
            "Epoch 2 Batch 50 Loss 1.7028 Accuracy 0.3825\n",
            "Epoch 2 Batch 100 Loss 1.7000 Accuracy 0.3830\n",
            "Epoch 2 Batch 150 Loss 1.6879 Accuracy 0.3850\n",
            "Epoch 2 Batch 200 Loss 1.6813 Accuracy 0.3858\n",
            "Epoch 2 Batch 250 Loss 1.6759 Accuracy 0.3869\n",
            "Epoch 2 Batch 300 Loss 1.6704 Accuracy 0.3873\n",
            "Epoch 2 Batch 350 Loss 1.6670 Accuracy 0.3874\n",
            "Epoch 2 Batch 400 Loss 1.6603 Accuracy 0.3884\n",
            "Epoch 2 Batch 450 Loss 1.6541 Accuracy 0.3889\n",
            "Epoch 2 Batch 500 Loss 1.6520 Accuracy 0.3891\n",
            "Epoch 2 Batch 550 Loss 1.6483 Accuracy 0.3895\n",
            "Epoch 2 Batch 600 Loss 1.6472 Accuracy 0.3896\n",
            "Epoch 2 Batch 650 Loss 1.6423 Accuracy 0.3901\n",
            "Epoch 2 Batch 700 Loss 1.6379 Accuracy 0.3910\n",
            "Epoch 2 Batch 750 Loss 1.6327 Accuracy 0.3922\n",
            "Epoch 2 Batch 800 Loss 1.6293 Accuracy 0.3929\n",
            "Epoch 2 Batch 850 Loss 1.6272 Accuracy 0.3936\n",
            "Epoch 2 Batch 900 Loss 1.6255 Accuracy 0.3940\n",
            "Epoch 2 Batch 950 Loss 1.6222 Accuracy 0.3942\n",
            "Epoch 2 Batch 1000 Loss 1.6184 Accuracy 0.3944\n",
            "Epoch 2 Batch 1050 Loss 1.6153 Accuracy 0.3946\n",
            "Epoch 2 Batch 1100 Loss 1.6114 Accuracy 0.3949\n",
            "Epoch 2 Batch 1150 Loss 1.6073 Accuracy 0.3951\n",
            "Epoch 2 Batch 1200 Loss 1.6034 Accuracy 0.3956\n",
            "Epoch 2 Batch 1250 Loss 1.5999 Accuracy 0.3961\n",
            "Epoch 2 Batch 1300 Loss 1.5952 Accuracy 0.3966\n",
            "Epoch 2 Batch 1350 Loss 1.5914 Accuracy 0.3974\n",
            "Epoch 2 Batch 1400 Loss 1.5871 Accuracy 0.3982\n",
            "Epoch 2 Batch 1450 Loss 1.5840 Accuracy 0.3991\n",
            "Epoch 2 Batch 1500 Loss 1.5798 Accuracy 0.4000\n",
            "Epoch 2 Batch 1550 Loss 1.5757 Accuracy 0.4010\n",
            "Epoch 2 Batch 1600 Loss 1.5716 Accuracy 0.4021\n",
            "Epoch 2 Batch 1650 Loss 1.5683 Accuracy 0.4032\n",
            "Epoch 2 Batch 1700 Loss 1.5643 Accuracy 0.4045\n",
            "Epoch 2 Batch 1750 Loss 1.5608 Accuracy 0.4054\n",
            "Epoch 2 Batch 1800 Loss 1.5570 Accuracy 0.4065\n",
            "Epoch 2 Batch 1850 Loss 1.5534 Accuracy 0.4074\n",
            "Epoch 2 Batch 1900 Loss 1.5502 Accuracy 0.4083\n",
            "Epoch 2 Batch 1950 Loss 1.5461 Accuracy 0.4092\n",
            "Epoch 2 Batch 2000 Loss 1.5428 Accuracy 0.4100\n",
            "Epoch 2 Batch 2050 Loss 1.5385 Accuracy 0.4107\n",
            "Epoch 2 Batch 2100 Loss 1.5341 Accuracy 0.4112\n",
            "Epoch 2 Batch 2150 Loss 1.5296 Accuracy 0.4116\n",
            "Epoch 2 Batch 2200 Loss 1.5247 Accuracy 0.4121\n",
            "Epoch 2 Batch 2250 Loss 1.5187 Accuracy 0.4125\n",
            "Epoch 2 Batch 2300 Loss 1.5146 Accuracy 0.4129\n",
            "Epoch 2 Batch 2350 Loss 1.5101 Accuracy 0.4134\n",
            "Epoch 2 Batch 2400 Loss 1.5055 Accuracy 0.4139\n",
            "Epoch 2 Batch 2450 Loss 1.5010 Accuracy 0.4145\n",
            "Epoch 2 Batch 2500 Loss 1.4955 Accuracy 0.4151\n",
            "Epoch 2 Batch 2550 Loss 1.4906 Accuracy 0.4156\n",
            "Epoch 2 Batch 2600 Loss 1.4862 Accuracy 0.4162\n",
            "Epoch 2 Batch 2650 Loss 1.4817 Accuracy 0.4168\n",
            "Epoch 2 Batch 2700 Loss 1.4770 Accuracy 0.4175\n",
            "Epoch 2 Batch 2750 Loss 1.4724 Accuracy 0.4181\n",
            "Epoch 2 Batch 2800 Loss 1.4690 Accuracy 0.4186\n",
            "Epoch 2 Batch 2850 Loss 1.4646 Accuracy 0.4193\n",
            "Epoch 2 Batch 2900 Loss 1.4609 Accuracy 0.4197\n",
            "Epoch 2 Batch 2950 Loss 1.4569 Accuracy 0.4203\n",
            "Epoch 2 Batch 3000 Loss 1.4539 Accuracy 0.4209\n",
            "Epoch 2 Batch 3050 Loss 1.4501 Accuracy 0.4215\n",
            "Epoch 2 Batch 3100 Loss 1.4462 Accuracy 0.4220\n",
            "Epoch 2 Batch 3150 Loss 1.4423 Accuracy 0.4225\n",
            "Epoch 2 Batch 3200 Loss 1.4389 Accuracy 0.4230\n",
            "Epoch 2 Batch 3250 Loss 1.4354 Accuracy 0.4234\n",
            "Epoch 2 Batch 3300 Loss 1.4319 Accuracy 0.4239\n",
            "Epoch 2 Batch 3350 Loss 1.4280 Accuracy 0.4245\n",
            "Epoch 2 Batch 3400 Loss 1.4242 Accuracy 0.4250\n",
            "Epoch 2 Batch 3450 Loss 1.4205 Accuracy 0.4255\n",
            "Epoch 2 Batch 3500 Loss 1.4169 Accuracy 0.4260\n",
            "Epoch 2 Batch 3550 Loss 1.4134 Accuracy 0.4266\n",
            "Epoch 2 Batch 3600 Loss 1.4098 Accuracy 0.4272\n",
            "Epoch 2 Batch 3650 Loss 1.4065 Accuracy 0.4278\n",
            "Epoch 2 Batch 3700 Loss 1.4031 Accuracy 0.4283\n",
            "Epoch 2 Batch 3750 Loss 1.4000 Accuracy 0.4289\n",
            "Epoch 2 Batch 3800 Loss 1.3969 Accuracy 0.4294\n",
            "Epoch 2 Batch 3850 Loss 1.3943 Accuracy 0.4299\n",
            "Epoch 2 Batch 3900 Loss 1.3916 Accuracy 0.4304\n",
            "Epoch 2 Batch 3950 Loss 1.3885 Accuracy 0.4310\n",
            "Epoch 2 Batch 4000 Loss 1.3855 Accuracy 0.4315\n",
            "Epoch 2 Batch 4050 Loss 1.3826 Accuracy 0.4320\n",
            "Epoch 2 Batch 4100 Loss 1.3800 Accuracy 0.4325\n",
            "Epoch 2 Batch 4150 Loss 1.3780 Accuracy 0.4328\n",
            "Epoch 2 Batch 4200 Loss 1.3766 Accuracy 0.4330\n",
            "Epoch 2 Batch 4250 Loss 1.3758 Accuracy 0.4332\n",
            "Epoch 2 Batch 4300 Loss 1.3755 Accuracy 0.4334\n",
            "Epoch 2 Batch 4350 Loss 1.3753 Accuracy 0.4335\n",
            "Epoch 2 Batch 4400 Loss 1.3750 Accuracy 0.4336\n",
            "Epoch 2 Batch 4450 Loss 1.3751 Accuracy 0.4336\n",
            "Epoch 2 Batch 4500 Loss 1.3750 Accuracy 0.4336\n",
            "Epoch 2 Batch 4550 Loss 1.3751 Accuracy 0.4336\n",
            "Epoch 2 Batch 4600 Loss 1.3750 Accuracy 0.4337\n",
            "Epoch 2 Batch 4650 Loss 1.3751 Accuracy 0.4336\n",
            "Epoch 2 Batch 4700 Loss 1.3753 Accuracy 0.4337\n",
            "Epoch 2 Batch 4750 Loss 1.3753 Accuracy 0.4338\n",
            "Epoch 2 Batch 4800 Loss 1.3749 Accuracy 0.4338\n",
            "Epoch 2 Batch 4850 Loss 1.3745 Accuracy 0.4338\n",
            "Epoch 2 Batch 4900 Loss 1.3742 Accuracy 0.4338\n",
            "Epoch 2 Batch 4950 Loss 1.3743 Accuracy 0.4338\n",
            "Epoch 2 Batch 5000 Loss 1.3743 Accuracy 0.4338\n",
            "Epoch 2 Batch 5050 Loss 1.3743 Accuracy 0.4337\n",
            "Epoch 2 Batch 5100 Loss 1.3742 Accuracy 0.4337\n",
            "Epoch 2 Batch 5150 Loss 1.3742 Accuracy 0.4336\n",
            "Epoch 2 Batch 5200 Loss 1.3740 Accuracy 0.4336\n",
            "Epoch 2 Batch 5250 Loss 1.3739 Accuracy 0.4335\n",
            "Epoch 2 Batch 5300 Loss 1.3738 Accuracy 0.4334\n",
            "Epoch 2 Batch 5350 Loss 1.3736 Accuracy 0.4334\n",
            "Epoch 2 Batch 5400 Loss 1.3736 Accuracy 0.4333\n",
            "Epoch 2 Batch 5450 Loss 1.3734 Accuracy 0.4332\n",
            "Epoch 2 Batch 5500 Loss 1.3731 Accuracy 0.4332\n",
            "Epoch 2 Batch 5550 Loss 1.3729 Accuracy 0.4331\n",
            "Epoch 2 Batch 5600 Loss 1.3726 Accuracy 0.4331\n",
            "Epoch 2 Batch 5650 Loss 1.3722 Accuracy 0.4331\n",
            "Epoch 2 Batch 5700 Loss 1.3719 Accuracy 0.4331\n",
            "Saving checkpoint for epoch 2 at ./drive/My Drive/Colab Notebooks/ModernNLP/Transformer/checkpoint/ckpt-2\n",
            "Time taken for 1 epoch: 1490.1270551681519 secs\n",
            "\n",
            "Start of epoch 3\n",
            "Epoch 3 Batch 0 Loss 1.3044 Accuracy 0.4112\n",
            "Epoch 3 Batch 50 Loss 1.3625 Accuracy 0.4340\n",
            "Epoch 3 Batch 100 Loss 1.3508 Accuracy 0.4360\n",
            "Epoch 3 Batch 150 Loss 1.3375 Accuracy 0.4359\n",
            "Epoch 3 Batch 200 Loss 1.3444 Accuracy 0.4377\n",
            "Epoch 3 Batch 250 Loss 1.3389 Accuracy 0.4382\n",
            "Epoch 3 Batch 300 Loss 1.3357 Accuracy 0.4385\n",
            "Epoch 3 Batch 350 Loss 1.3311 Accuracy 0.4380\n",
            "Epoch 3 Batch 400 Loss 1.3287 Accuracy 0.4384\n",
            "Epoch 3 Batch 450 Loss 1.3280 Accuracy 0.4380\n",
            "Epoch 3 Batch 500 Loss 1.3247 Accuracy 0.4382\n",
            "Epoch 3 Batch 550 Loss 1.3246 Accuracy 0.4383\n",
            "Epoch 3 Batch 600 Loss 1.3241 Accuracy 0.4379\n",
            "Epoch 3 Batch 650 Loss 1.3225 Accuracy 0.4380\n",
            "Epoch 3 Batch 700 Loss 1.3198 Accuracy 0.4385\n",
            "Epoch 3 Batch 750 Loss 1.3161 Accuracy 0.4390\n",
            "Epoch 3 Batch 800 Loss 1.3166 Accuracy 0.4391\n",
            "Epoch 3 Batch 850 Loss 1.3145 Accuracy 0.4393\n",
            "Epoch 3 Batch 900 Loss 1.3119 Accuracy 0.4395\n",
            "Epoch 3 Batch 950 Loss 1.3115 Accuracy 0.4399\n",
            "Epoch 3 Batch 1000 Loss 1.3095 Accuracy 0.4401\n",
            "Epoch 3 Batch 1050 Loss 1.3058 Accuracy 0.4400\n",
            "Epoch 3 Batch 1100 Loss 1.3040 Accuracy 0.4402\n",
            "Epoch 3 Batch 1150 Loss 1.3021 Accuracy 0.4404\n",
            "Epoch 3 Batch 1200 Loss 1.3005 Accuracy 0.4408\n",
            "Epoch 3 Batch 1250 Loss 1.2986 Accuracy 0.4410\n",
            "Epoch 3 Batch 1300 Loss 1.2964 Accuracy 0.4414\n",
            "Epoch 3 Batch 1350 Loss 1.2937 Accuracy 0.4419\n",
            "Epoch 3 Batch 1400 Loss 1.2917 Accuracy 0.4426\n",
            "Epoch 3 Batch 1450 Loss 1.2878 Accuracy 0.4434\n",
            "Epoch 3 Batch 1500 Loss 1.2852 Accuracy 0.4441\n",
            "Epoch 3 Batch 1550 Loss 1.2818 Accuracy 0.4450\n",
            "Epoch 3 Batch 1600 Loss 1.2796 Accuracy 0.4459\n",
            "Epoch 3 Batch 1650 Loss 1.2775 Accuracy 0.4467\n",
            "Epoch 3 Batch 1700 Loss 1.2741 Accuracy 0.4476\n",
            "Epoch 3 Batch 1750 Loss 1.2715 Accuracy 0.4485\n",
            "Epoch 3 Batch 1800 Loss 1.2692 Accuracy 0.4492\n",
            "Epoch 3 Batch 1850 Loss 1.2665 Accuracy 0.4501\n",
            "Epoch 3 Batch 1900 Loss 1.2636 Accuracy 0.4509\n",
            "Epoch 3 Batch 1950 Loss 1.2616 Accuracy 0.4516\n",
            "Epoch 3 Batch 2000 Loss 1.2596 Accuracy 0.4523\n",
            "Epoch 3 Batch 2050 Loss 1.2570 Accuracy 0.4529\n",
            "Epoch 3 Batch 2100 Loss 1.2543 Accuracy 0.4532\n",
            "Epoch 3 Batch 2150 Loss 1.2513 Accuracy 0.4536\n",
            "Epoch 3 Batch 2200 Loss 1.2477 Accuracy 0.4538\n",
            "Epoch 3 Batch 2250 Loss 1.2439 Accuracy 0.4540\n",
            "Epoch 3 Batch 2300 Loss 1.2397 Accuracy 0.4543\n",
            "Epoch 3 Batch 2350 Loss 1.2365 Accuracy 0.4546\n",
            "Epoch 3 Batch 2400 Loss 1.2335 Accuracy 0.4550\n",
            "Epoch 3 Batch 2450 Loss 1.2303 Accuracy 0.4553\n",
            "Epoch 3 Batch 2500 Loss 1.2269 Accuracy 0.4558\n",
            "Epoch 3 Batch 2550 Loss 1.2234 Accuracy 0.4562\n",
            "Epoch 3 Batch 2600 Loss 1.2195 Accuracy 0.4566\n",
            "Epoch 3 Batch 2650 Loss 1.2168 Accuracy 0.4570\n",
            "Epoch 3 Batch 2700 Loss 1.2134 Accuracy 0.4574\n",
            "Epoch 3 Batch 2750 Loss 1.2108 Accuracy 0.4578\n",
            "Epoch 3 Batch 2800 Loss 1.2084 Accuracy 0.4580\n",
            "Epoch 3 Batch 2850 Loss 1.2057 Accuracy 0.4584\n",
            "Epoch 3 Batch 2900 Loss 1.2032 Accuracy 0.4586\n",
            "Epoch 3 Batch 2950 Loss 1.2006 Accuracy 0.4590\n",
            "Epoch 3 Batch 3000 Loss 1.1981 Accuracy 0.4594\n",
            "Epoch 3 Batch 3050 Loss 1.1954 Accuracy 0.4598\n",
            "Epoch 3 Batch 3100 Loss 1.1931 Accuracy 0.4602\n",
            "Epoch 3 Batch 3150 Loss 1.1909 Accuracy 0.4604\n",
            "Epoch 3 Batch 3200 Loss 1.1886 Accuracy 0.4608\n",
            "Epoch 3 Batch 3250 Loss 1.1864 Accuracy 0.4610\n",
            "Epoch 3 Batch 3300 Loss 1.1835 Accuracy 0.4614\n",
            "Epoch 3 Batch 3350 Loss 1.1809 Accuracy 0.4618\n",
            "Epoch 3 Batch 3400 Loss 1.1782 Accuracy 0.4622\n",
            "Epoch 3 Batch 3450 Loss 1.1758 Accuracy 0.4626\n",
            "Epoch 3 Batch 3500 Loss 1.1735 Accuracy 0.4630\n",
            "Epoch 3 Batch 3550 Loss 1.1711 Accuracy 0.4634\n",
            "Epoch 3 Batch 3600 Loss 1.1688 Accuracy 0.4638\n",
            "Epoch 3 Batch 3650 Loss 1.1666 Accuracy 0.4642\n",
            "Epoch 3 Batch 3700 Loss 1.1644 Accuracy 0.4645\n",
            "Epoch 3 Batch 3750 Loss 1.1619 Accuracy 0.4649\n",
            "Epoch 3 Batch 3800 Loss 1.1597 Accuracy 0.4653\n",
            "Epoch 3 Batch 3850 Loss 1.1584 Accuracy 0.4657\n",
            "Epoch 3 Batch 3900 Loss 1.1562 Accuracy 0.4661\n",
            "Epoch 3 Batch 3950 Loss 1.1544 Accuracy 0.4665\n",
            "Epoch 3 Batch 4000 Loss 1.1527 Accuracy 0.4669\n",
            "Epoch 3 Batch 4050 Loss 1.1510 Accuracy 0.4672\n",
            "Epoch 3 Batch 4100 Loss 1.1492 Accuracy 0.4676\n",
            "Epoch 3 Batch 4150 Loss 1.1488 Accuracy 0.4677\n",
            "Epoch 3 Batch 4200 Loss 1.1487 Accuracy 0.4679\n",
            "Epoch 3 Batch 4250 Loss 1.1490 Accuracy 0.4679\n",
            "Epoch 3 Batch 4300 Loss 1.1491 Accuracy 0.4679\n",
            "Epoch 3 Batch 4350 Loss 1.1494 Accuracy 0.4678\n",
            "Epoch 3 Batch 4400 Loss 1.1502 Accuracy 0.4677\n",
            "Epoch 3 Batch 4450 Loss 1.1512 Accuracy 0.4676\n",
            "Epoch 3 Batch 4500 Loss 1.1520 Accuracy 0.4674\n",
            "Epoch 3 Batch 4550 Loss 1.1531 Accuracy 0.4673\n",
            "Epoch 3 Batch 4600 Loss 1.1539 Accuracy 0.4672\n",
            "Epoch 3 Batch 4650 Loss 1.1548 Accuracy 0.4670\n",
            "Epoch 3 Batch 4700 Loss 1.1558 Accuracy 0.4669\n",
            "Epoch 3 Batch 4750 Loss 1.1564 Accuracy 0.4668\n",
            "Epoch 3 Batch 4800 Loss 1.1574 Accuracy 0.4667\n",
            "Epoch 3 Batch 4850 Loss 1.1582 Accuracy 0.4666\n",
            "Epoch 3 Batch 4900 Loss 1.1592 Accuracy 0.4665\n",
            "Epoch 3 Batch 4950 Loss 1.1600 Accuracy 0.4664\n",
            "Epoch 3 Batch 5000 Loss 1.1607 Accuracy 0.4663\n",
            "Epoch 3 Batch 5050 Loss 1.1616 Accuracy 0.4661\n",
            "Epoch 3 Batch 5100 Loss 1.1626 Accuracy 0.4659\n",
            "Epoch 3 Batch 5150 Loss 1.1636 Accuracy 0.4657\n",
            "Epoch 3 Batch 5200 Loss 1.1644 Accuracy 0.4656\n",
            "Epoch 3 Batch 5250 Loss 1.1653 Accuracy 0.4654\n",
            "Epoch 3 Batch 5300 Loss 1.1657 Accuracy 0.4651\n",
            "Epoch 3 Batch 5350 Loss 1.1660 Accuracy 0.4649\n",
            "Epoch 3 Batch 5400 Loss 1.1667 Accuracy 0.4647\n",
            "Epoch 3 Batch 5450 Loss 1.1674 Accuracy 0.4646\n",
            "Epoch 3 Batch 5500 Loss 1.1680 Accuracy 0.4644\n",
            "Epoch 3 Batch 5550 Loss 1.1685 Accuracy 0.4642\n",
            "Epoch 3 Batch 5600 Loss 1.1693 Accuracy 0.4641\n",
            "Epoch 3 Batch 5650 Loss 1.1697 Accuracy 0.4639\n",
            "Epoch 3 Batch 5700 Loss 1.1703 Accuracy 0.4637\n",
            "Saving checkpoint for epoch 3 at ./drive/My Drive/Colab Notebooks/ModernNLP/Transformer/checkpoint/ckpt-3\n",
            "Time taken for 1 epoch: 1511.3557305335999 secs\n",
            "\n",
            "Start of epoch 4\n",
            "Epoch 4 Batch 0 Loss 1.0989 Accuracy 0.4655\n",
            "Epoch 4 Batch 50 Loss 1.2278 Accuracy 0.4535\n",
            "Epoch 4 Batch 100 Loss 1.2235 Accuracy 0.4536\n",
            "Epoch 4 Batch 150 Loss 1.2161 Accuracy 0.4543\n",
            "Epoch 4 Batch 200 Loss 1.2204 Accuracy 0.4547\n",
            "Epoch 4 Batch 250 Loss 1.2200 Accuracy 0.4543\n",
            "Epoch 4 Batch 300 Loss 1.2205 Accuracy 0.4549\n",
            "Epoch 4 Batch 350 Loss 1.2209 Accuracy 0.4557\n",
            "Epoch 4 Batch 400 Loss 1.2176 Accuracy 0.4552\n",
            "Epoch 4 Batch 450 Loss 1.2151 Accuracy 0.4557\n",
            "Epoch 4 Batch 500 Loss 1.2138 Accuracy 0.4555\n",
            "Epoch 4 Batch 550 Loss 1.2092 Accuracy 0.4557\n",
            "Epoch 4 Batch 600 Loss 1.2098 Accuracy 0.4556\n",
            "Epoch 4 Batch 650 Loss 1.2091 Accuracy 0.4558\n",
            "Epoch 4 Batch 700 Loss 1.2080 Accuracy 0.4562\n",
            "Epoch 4 Batch 750 Loss 1.2054 Accuracy 0.4562\n",
            "Epoch 4 Batch 800 Loss 1.2033 Accuracy 0.4565\n",
            "Epoch 4 Batch 850 Loss 1.2034 Accuracy 0.4566\n",
            "Epoch 4 Batch 900 Loss 1.2037 Accuracy 0.4565\n",
            "Epoch 4 Batch 950 Loss 1.2023 Accuracy 0.4565\n",
            "Epoch 4 Batch 1000 Loss 1.2011 Accuracy 0.4566\n",
            "Epoch 4 Batch 1050 Loss 1.1994 Accuracy 0.4566\n",
            "Epoch 4 Batch 1100 Loss 1.1977 Accuracy 0.4567\n",
            "Epoch 4 Batch 1150 Loss 1.1962 Accuracy 0.4569\n",
            "Epoch 4 Batch 1200 Loss 1.1937 Accuracy 0.4571\n",
            "Epoch 4 Batch 1250 Loss 1.1914 Accuracy 0.4575\n",
            "Epoch 4 Batch 1300 Loss 1.1899 Accuracy 0.4580\n",
            "Epoch 4 Batch 1350 Loss 1.1881 Accuracy 0.4584\n",
            "Epoch 4 Batch 1400 Loss 1.1845 Accuracy 0.4592\n",
            "Epoch 4 Batch 1450 Loss 1.1822 Accuracy 0.4600\n",
            "Epoch 4 Batch 1500 Loss 1.1793 Accuracy 0.4607\n",
            "Epoch 4 Batch 1550 Loss 1.1766 Accuracy 0.4615\n",
            "Epoch 4 Batch 1600 Loss 1.1739 Accuracy 0.4623\n",
            "Epoch 4 Batch 1650 Loss 1.1719 Accuracy 0.4630\n",
            "Epoch 4 Batch 1700 Loss 1.1690 Accuracy 0.4636\n",
            "Epoch 4 Batch 1750 Loss 1.1666 Accuracy 0.4645\n",
            "Epoch 4 Batch 1800 Loss 1.1642 Accuracy 0.4655\n",
            "Epoch 4 Batch 1850 Loss 1.1620 Accuracy 0.4661\n",
            "Epoch 4 Batch 1900 Loss 1.1598 Accuracy 0.4669\n",
            "Epoch 4 Batch 1950 Loss 1.1583 Accuracy 0.4677\n",
            "Epoch 4 Batch 2000 Loss 1.1560 Accuracy 0.4682\n",
            "Epoch 4 Batch 2050 Loss 1.1534 Accuracy 0.4688\n",
            "Epoch 4 Batch 2100 Loss 1.1504 Accuracy 0.4692\n",
            "Epoch 4 Batch 2150 Loss 1.1475 Accuracy 0.4695\n",
            "Epoch 4 Batch 2200 Loss 1.1443 Accuracy 0.4699\n",
            "Epoch 4 Batch 2250 Loss 1.1410 Accuracy 0.4701\n",
            "Epoch 4 Batch 2300 Loss 1.1377 Accuracy 0.4704\n",
            "Epoch 4 Batch 2350 Loss 1.1346 Accuracy 0.4707\n",
            "Epoch 4 Batch 2400 Loss 1.1317 Accuracy 0.4709\n",
            "Epoch 4 Batch 2450 Loss 1.1284 Accuracy 0.4712\n",
            "Epoch 4 Batch 2500 Loss 1.1252 Accuracy 0.4715\n",
            "Epoch 4 Batch 2550 Loss 1.1223 Accuracy 0.4719\n",
            "Epoch 4 Batch 2600 Loss 1.1190 Accuracy 0.4723\n",
            "Epoch 4 Batch 2650 Loss 1.1161 Accuracy 0.4727\n",
            "Epoch 4 Batch 2700 Loss 1.1135 Accuracy 0.4731\n",
            "Epoch 4 Batch 2750 Loss 1.1109 Accuracy 0.4734\n",
            "Epoch 4 Batch 2800 Loss 1.1084 Accuracy 0.4737\n",
            "Epoch 4 Batch 2850 Loss 1.1057 Accuracy 0.4740\n",
            "Epoch 4 Batch 2900 Loss 1.1033 Accuracy 0.4744\n",
            "Epoch 4 Batch 2950 Loss 1.1007 Accuracy 0.4747\n",
            "Epoch 4 Batch 3000 Loss 1.0986 Accuracy 0.4750\n",
            "Epoch 4 Batch 3050 Loss 1.0968 Accuracy 0.4752\n",
            "Epoch 4 Batch 3100 Loss 1.0942 Accuracy 0.4756\n",
            "Epoch 4 Batch 3150 Loss 1.0922 Accuracy 0.4759\n",
            "Epoch 4 Batch 3200 Loss 1.0901 Accuracy 0.4762\n",
            "Epoch 4 Batch 3250 Loss 1.0881 Accuracy 0.4766\n",
            "Epoch 4 Batch 3300 Loss 1.0858 Accuracy 0.4768\n",
            "Epoch 4 Batch 3350 Loss 1.0839 Accuracy 0.4771\n",
            "Epoch 4 Batch 3400 Loss 1.0819 Accuracy 0.4774\n",
            "Epoch 4 Batch 3450 Loss 1.0800 Accuracy 0.4777\n",
            "Epoch 4 Batch 3500 Loss 1.0779 Accuracy 0.4780\n",
            "Epoch 4 Batch 3550 Loss 1.0762 Accuracy 0.4784\n",
            "Epoch 4 Batch 3600 Loss 1.0742 Accuracy 0.4788\n",
            "Epoch 4 Batch 3650 Loss 1.0719 Accuracy 0.4792\n",
            "Epoch 4 Batch 3700 Loss 1.0700 Accuracy 0.4796\n",
            "Epoch 4 Batch 3750 Loss 1.0682 Accuracy 0.4800\n",
            "Epoch 4 Batch 3800 Loss 1.0662 Accuracy 0.4804\n",
            "Epoch 4 Batch 3850 Loss 1.0644 Accuracy 0.4807\n",
            "Epoch 4 Batch 3900 Loss 1.0631 Accuracy 0.4810\n",
            "Epoch 4 Batch 3950 Loss 1.0615 Accuracy 0.4814\n",
            "Epoch 4 Batch 4000 Loss 1.0600 Accuracy 0.4817\n",
            "Epoch 4 Batch 4050 Loss 1.0583 Accuracy 0.4819\n",
            "Epoch 4 Batch 4100 Loss 1.0573 Accuracy 0.4822\n",
            "Epoch 4 Batch 4150 Loss 1.0567 Accuracy 0.4823\n",
            "Epoch 4 Batch 4200 Loss 1.0565 Accuracy 0.4824\n",
            "Epoch 4 Batch 4250 Loss 1.0568 Accuracy 0.4824\n",
            "Epoch 4 Batch 4300 Loss 1.0576 Accuracy 0.4823\n",
            "Epoch 4 Batch 4350 Loss 1.0582 Accuracy 0.4822\n",
            "Epoch 4 Batch 4400 Loss 1.0593 Accuracy 0.4822\n",
            "Epoch 4 Batch 4450 Loss 1.0602 Accuracy 0.4820\n",
            "Epoch 4 Batch 4500 Loss 1.0611 Accuracy 0.4819\n",
            "Epoch 4 Batch 4550 Loss 1.0621 Accuracy 0.4817\n",
            "Epoch 4 Batch 4600 Loss 1.0634 Accuracy 0.4815\n",
            "Epoch 4 Batch 4650 Loss 1.0645 Accuracy 0.4813\n",
            "Epoch 4 Batch 4700 Loss 1.0659 Accuracy 0.4812\n",
            "Epoch 4 Batch 4750 Loss 1.0668 Accuracy 0.4810\n",
            "Epoch 4 Batch 4800 Loss 1.0679 Accuracy 0.4809\n",
            "Epoch 4 Batch 4850 Loss 1.0690 Accuracy 0.4808\n",
            "Epoch 4 Batch 4900 Loss 1.0698 Accuracy 0.4806\n",
            "Epoch 4 Batch 4950 Loss 1.0710 Accuracy 0.4805\n",
            "Epoch 4 Batch 5000 Loss 1.0719 Accuracy 0.4803\n",
            "Epoch 4 Batch 5050 Loss 1.0730 Accuracy 0.4802\n",
            "Epoch 4 Batch 5100 Loss 1.0741 Accuracy 0.4800\n",
            "Epoch 4 Batch 5150 Loss 1.0753 Accuracy 0.4798\n",
            "Epoch 4 Batch 5200 Loss 1.0765 Accuracy 0.4795\n",
            "Epoch 4 Batch 5250 Loss 1.0773 Accuracy 0.4793\n",
            "Epoch 4 Batch 5300 Loss 1.0785 Accuracy 0.4790\n",
            "Epoch 4 Batch 5350 Loss 1.0795 Accuracy 0.4787\n",
            "Epoch 4 Batch 5400 Loss 1.0802 Accuracy 0.4785\n",
            "Epoch 4 Batch 5450 Loss 1.0811 Accuracy 0.4782\n",
            "Epoch 4 Batch 5500 Loss 1.0821 Accuracy 0.4780\n",
            "Epoch 4 Batch 5550 Loss 1.0828 Accuracy 0.4778\n",
            "Epoch 4 Batch 5600 Loss 1.0834 Accuracy 0.4776\n",
            "Epoch 4 Batch 5650 Loss 1.0840 Accuracy 0.4774\n",
            "Epoch 4 Batch 5700 Loss 1.0846 Accuracy 0.4772\n",
            "Saving checkpoint for epoch 4 at ./drive/My Drive/Colab Notebooks/ModernNLP/Transformer/checkpoint/ckpt-4\n",
            "Time taken for 1 epoch: 1488.1140625476837 secs\n",
            "\n",
            "Start of epoch 5\n",
            "Epoch 5 Batch 0 Loss 0.9895 Accuracy 0.4400\n",
            "Epoch 5 Batch 50 Loss 1.1483 Accuracy 0.4646\n",
            "Epoch 5 Batch 100 Loss 1.1652 Accuracy 0.4620\n",
            "Epoch 5 Batch 150 Loss 1.1639 Accuracy 0.4631\n",
            "Epoch 5 Batch 200 Loss 1.1601 Accuracy 0.4638\n",
            "Epoch 5 Batch 250 Loss 1.1618 Accuracy 0.4644\n",
            "Epoch 5 Batch 300 Loss 1.1615 Accuracy 0.4648\n",
            "Epoch 5 Batch 350 Loss 1.1599 Accuracy 0.4645\n",
            "Epoch 5 Batch 400 Loss 1.1547 Accuracy 0.4642\n",
            "Epoch 5 Batch 450 Loss 1.1515 Accuracy 0.4645\n",
            "Epoch 5 Batch 500 Loss 1.1504 Accuracy 0.4646\n",
            "Epoch 5 Batch 550 Loss 1.1493 Accuracy 0.4645\n",
            "Epoch 5 Batch 600 Loss 1.1476 Accuracy 0.4650\n",
            "Epoch 5 Batch 650 Loss 1.1468 Accuracy 0.4653\n",
            "Epoch 5 Batch 700 Loss 1.1464 Accuracy 0.4657\n",
            "Epoch 5 Batch 750 Loss 1.1464 Accuracy 0.4655\n",
            "Epoch 5 Batch 800 Loss 1.1457 Accuracy 0.4656\n",
            "Epoch 5 Batch 850 Loss 1.1445 Accuracy 0.4658\n",
            "Epoch 5 Batch 900 Loss 1.1426 Accuracy 0.4659\n",
            "Epoch 5 Batch 950 Loss 1.1397 Accuracy 0.4661\n",
            "Epoch 5 Batch 1000 Loss 1.1372 Accuracy 0.4662\n",
            "Epoch 5 Batch 1050 Loss 1.1359 Accuracy 0.4664\n",
            "Epoch 5 Batch 1100 Loss 1.1352 Accuracy 0.4666\n",
            "Epoch 5 Batch 1150 Loss 1.1350 Accuracy 0.4668\n",
            "Epoch 5 Batch 1200 Loss 1.1347 Accuracy 0.4672\n",
            "Epoch 5 Batch 1250 Loss 1.1324 Accuracy 0.4675\n",
            "Epoch 5 Batch 1300 Loss 1.1299 Accuracy 0.4680\n",
            "Epoch 5 Batch 1350 Loss 1.1275 Accuracy 0.4685\n",
            "Epoch 5 Batch 1400 Loss 1.1253 Accuracy 0.4692\n",
            "Epoch 5 Batch 1450 Loss 1.1228 Accuracy 0.4700\n",
            "Epoch 5 Batch 1500 Loss 1.1198 Accuracy 0.4709\n",
            "Epoch 5 Batch 1550 Loss 1.1172 Accuracy 0.4717\n",
            "Epoch 5 Batch 1600 Loss 1.1147 Accuracy 0.4726\n",
            "Epoch 5 Batch 1650 Loss 1.1128 Accuracy 0.4732\n",
            "Epoch 5 Batch 1700 Loss 1.1101 Accuracy 0.4739\n",
            "Epoch 5 Batch 1750 Loss 1.1071 Accuracy 0.4747\n",
            "Epoch 5 Batch 1800 Loss 1.1050 Accuracy 0.4755\n",
            "Epoch 5 Batch 1850 Loss 1.1029 Accuracy 0.4763\n",
            "Epoch 5 Batch 1900 Loss 1.1007 Accuracy 0.4770\n",
            "Epoch 5 Batch 1950 Loss 1.0986 Accuracy 0.4778\n",
            "Epoch 5 Batch 2000 Loss 1.0959 Accuracy 0.4783\n",
            "Epoch 5 Batch 2050 Loss 1.0936 Accuracy 0.4788\n",
            "Epoch 5 Batch 2100 Loss 1.0906 Accuracy 0.4792\n",
            "Epoch 5 Batch 2150 Loss 1.0877 Accuracy 0.4796\n",
            "Epoch 5 Batch 2200 Loss 1.0840 Accuracy 0.4799\n",
            "Epoch 5 Batch 2250 Loss 1.0811 Accuracy 0.4802\n",
            "Epoch 5 Batch 2300 Loss 1.0777 Accuracy 0.4805\n",
            "Epoch 5 Batch 2350 Loss 1.0748 Accuracy 0.4806\n",
            "Epoch 5 Batch 2400 Loss 1.0720 Accuracy 0.4807\n",
            "Epoch 5 Batch 2450 Loss 1.0687 Accuracy 0.4811\n",
            "Epoch 5 Batch 2500 Loss 1.0656 Accuracy 0.4815\n",
            "Epoch 5 Batch 2550 Loss 1.0627 Accuracy 0.4818\n",
            "Epoch 5 Batch 2600 Loss 1.0599 Accuracy 0.4821\n",
            "Epoch 5 Batch 2650 Loss 1.0574 Accuracy 0.4823\n",
            "Epoch 5 Batch 2700 Loss 1.0549 Accuracy 0.4827\n",
            "Epoch 5 Batch 2750 Loss 1.0524 Accuracy 0.4830\n",
            "Epoch 5 Batch 2800 Loss 1.0499 Accuracy 0.4832\n",
            "Epoch 5 Batch 2850 Loss 1.0478 Accuracy 0.4835\n",
            "Epoch 5 Batch 2900 Loss 1.0459 Accuracy 0.4838\n",
            "Epoch 5 Batch 2950 Loss 1.0441 Accuracy 0.4842\n",
            "Epoch 5 Batch 3000 Loss 1.0418 Accuracy 0.4844\n",
            "Epoch 5 Batch 3050 Loss 1.0397 Accuracy 0.4846\n",
            "Epoch 5 Batch 3100 Loss 1.0378 Accuracy 0.4850\n",
            "Epoch 5 Batch 3150 Loss 1.0361 Accuracy 0.4852\n",
            "Epoch 5 Batch 3200 Loss 1.0343 Accuracy 0.4854\n",
            "Epoch 5 Batch 3250 Loss 1.0322 Accuracy 0.4856\n",
            "Epoch 5 Batch 3300 Loss 1.0301 Accuracy 0.4859\n",
            "Epoch 5 Batch 3350 Loss 1.0280 Accuracy 0.4862\n",
            "Epoch 5 Batch 3400 Loss 1.0260 Accuracy 0.4866\n",
            "Epoch 5 Batch 3450 Loss 1.0241 Accuracy 0.4869\n",
            "Epoch 5 Batch 3500 Loss 1.0218 Accuracy 0.4872\n",
            "Epoch 5 Batch 3550 Loss 1.0200 Accuracy 0.4875\n",
            "Epoch 5 Batch 3600 Loss 1.0181 Accuracy 0.4878\n",
            "Epoch 5 Batch 3650 Loss 1.0158 Accuracy 0.4881\n",
            "Epoch 5 Batch 3700 Loss 1.0142 Accuracy 0.4885\n",
            "Epoch 5 Batch 3750 Loss 1.0125 Accuracy 0.4888\n",
            "Epoch 5 Batch 3800 Loss 1.0108 Accuracy 0.4891\n",
            "Epoch 5 Batch 3850 Loss 1.0095 Accuracy 0.4895\n",
            "Epoch 5 Batch 3900 Loss 1.0083 Accuracy 0.4899\n",
            "Epoch 5 Batch 3950 Loss 1.0070 Accuracy 0.4902\n",
            "Epoch 5 Batch 4000 Loss 1.0053 Accuracy 0.4905\n",
            "Epoch 5 Batch 4050 Loss 1.0039 Accuracy 0.4908\n",
            "Epoch 5 Batch 4100 Loss 1.0025 Accuracy 0.4910\n",
            "Epoch 5 Batch 4150 Loss 1.0021 Accuracy 0.4912\n",
            "Epoch 5 Batch 4200 Loss 1.0022 Accuracy 0.4912\n",
            "Epoch 5 Batch 4250 Loss 1.0024 Accuracy 0.4912\n",
            "Epoch 5 Batch 4300 Loss 1.0032 Accuracy 0.4911\n",
            "Epoch 5 Batch 4350 Loss 1.0044 Accuracy 0.4909\n",
            "Epoch 5 Batch 4400 Loss 1.0054 Accuracy 0.4909\n",
            "Epoch 5 Batch 4450 Loss 1.0065 Accuracy 0.4908\n",
            "Epoch 5 Batch 4500 Loss 1.0078 Accuracy 0.4906\n",
            "Epoch 5 Batch 4550 Loss 1.0089 Accuracy 0.4905\n",
            "Epoch 5 Batch 4600 Loss 1.0100 Accuracy 0.4903\n",
            "Epoch 5 Batch 4650 Loss 1.0110 Accuracy 0.4902\n",
            "Epoch 5 Batch 4700 Loss 1.0125 Accuracy 0.4899\n",
            "Epoch 5 Batch 4750 Loss 1.0136 Accuracy 0.4898\n",
            "Epoch 5 Batch 4800 Loss 1.0149 Accuracy 0.4896\n",
            "Epoch 5 Batch 4850 Loss 1.0160 Accuracy 0.4894\n",
            "Epoch 5 Batch 4900 Loss 1.0172 Accuracy 0.4893\n",
            "Epoch 5 Batch 4950 Loss 1.0183 Accuracy 0.4891\n",
            "Epoch 5 Batch 5000 Loss 1.0194 Accuracy 0.4889\n",
            "Epoch 5 Batch 5050 Loss 1.0208 Accuracy 0.4887\n",
            "Epoch 5 Batch 5100 Loss 1.0223 Accuracy 0.4884\n",
            "Epoch 5 Batch 5150 Loss 1.0236 Accuracy 0.4882\n",
            "Epoch 5 Batch 5200 Loss 1.0248 Accuracy 0.4879\n",
            "Epoch 5 Batch 5250 Loss 1.0258 Accuracy 0.4877\n",
            "Epoch 5 Batch 5300 Loss 1.0270 Accuracy 0.4874\n",
            "Epoch 5 Batch 5350 Loss 1.0282 Accuracy 0.4871\n",
            "Epoch 5 Batch 5400 Loss 1.0290 Accuracy 0.4869\n",
            "Epoch 5 Batch 5450 Loss 1.0299 Accuracy 0.4866\n",
            "Epoch 5 Batch 5500 Loss 1.0307 Accuracy 0.4864\n",
            "Epoch 5 Batch 5550 Loss 1.0316 Accuracy 0.4861\n",
            "Epoch 5 Batch 5600 Loss 1.0323 Accuracy 0.4859\n",
            "Epoch 5 Batch 5650 Loss 1.0328 Accuracy 0.4858\n",
            "Epoch 5 Batch 5700 Loss 1.0337 Accuracy 0.4855\n",
            "Saving checkpoint for epoch 5 at ./drive/My Drive/Colab Notebooks/ModernNLP/Transformer/checkpoint/ckpt-5\n",
            "Time taken for 1 epoch: 1466.2387113571167 secs\n",
            "\n",
            "Start of epoch 6\n",
            "Epoch 6 Batch 0 Loss 1.0680 Accuracy 0.4507\n",
            "Epoch 6 Batch 50 Loss 1.1270 Accuracy 0.4683\n",
            "Epoch 6 Batch 100 Loss 1.1163 Accuracy 0.4710\n",
            "Epoch 6 Batch 150 Loss 1.1128 Accuracy 0.4721\n",
            "Epoch 6 Batch 200 Loss 1.1194 Accuracy 0.4727\n",
            "Epoch 6 Batch 250 Loss 1.1186 Accuracy 0.4722\n",
            "Epoch 6 Batch 300 Loss 1.1172 Accuracy 0.4715\n",
            "Epoch 6 Batch 350 Loss 1.1152 Accuracy 0.4721\n",
            "Epoch 6 Batch 400 Loss 1.1142 Accuracy 0.4722\n",
            "Epoch 6 Batch 450 Loss 1.1112 Accuracy 0.4722\n",
            "Epoch 6 Batch 500 Loss 1.1070 Accuracy 0.4722\n",
            "Epoch 6 Batch 550 Loss 1.1065 Accuracy 0.4724\n",
            "Epoch 6 Batch 600 Loss 1.1073 Accuracy 0.4719\n",
            "Epoch 6 Batch 650 Loss 1.1047 Accuracy 0.4721\n",
            "Epoch 6 Batch 700 Loss 1.1040 Accuracy 0.4725\n",
            "Epoch 6 Batch 750 Loss 1.1029 Accuracy 0.4728\n",
            "Epoch 6 Batch 800 Loss 1.1015 Accuracy 0.4732\n",
            "Epoch 6 Batch 850 Loss 1.0999 Accuracy 0.4732\n",
            "Epoch 6 Batch 900 Loss 1.0993 Accuracy 0.4733\n",
            "Epoch 6 Batch 950 Loss 1.0983 Accuracy 0.4731\n",
            "Epoch 6 Batch 1000 Loss 1.0962 Accuracy 0.4733\n",
            "Epoch 6 Batch 1050 Loss 1.0946 Accuracy 0.4732\n",
            "Epoch 6 Batch 1100 Loss 1.0938 Accuracy 0.4734\n",
            "Epoch 6 Batch 1150 Loss 1.0931 Accuracy 0.4735\n",
            "Epoch 6 Batch 1200 Loss 1.0919 Accuracy 0.4739\n",
            "Epoch 6 Batch 1250 Loss 1.0904 Accuracy 0.4743\n",
            "Epoch 6 Batch 1300 Loss 1.0880 Accuracy 0.4747\n",
            "Epoch 6 Batch 1350 Loss 1.0861 Accuracy 0.4751\n",
            "Epoch 6 Batch 1400 Loss 1.0841 Accuracy 0.4757\n",
            "Epoch 6 Batch 1450 Loss 1.0817 Accuracy 0.4765\n",
            "Epoch 6 Batch 1500 Loss 1.0790 Accuracy 0.4771\n",
            "Epoch 6 Batch 1550 Loss 1.0761 Accuracy 0.4778\n",
            "Epoch 6 Batch 1600 Loss 1.0742 Accuracy 0.4789\n",
            "Epoch 6 Batch 1650 Loss 1.0716 Accuracy 0.4797\n",
            "Epoch 6 Batch 1700 Loss 1.0688 Accuracy 0.4803\n",
            "Epoch 6 Batch 1750 Loss 1.0665 Accuracy 0.4809\n",
            "Epoch 6 Batch 1800 Loss 1.0647 Accuracy 0.4817\n",
            "Epoch 6 Batch 1850 Loss 1.0620 Accuracy 0.4825\n",
            "Epoch 6 Batch 1900 Loss 1.0596 Accuracy 0.4832\n",
            "Epoch 6 Batch 1950 Loss 1.0570 Accuracy 0.4841\n",
            "Epoch 6 Batch 2000 Loss 1.0545 Accuracy 0.4847\n",
            "Epoch 6 Batch 2050 Loss 1.0528 Accuracy 0.4851\n",
            "Epoch 6 Batch 2100 Loss 1.0497 Accuracy 0.4853\n",
            "Epoch 6 Batch 2150 Loss 1.0473 Accuracy 0.4857\n",
            "Epoch 6 Batch 2200 Loss 1.0440 Accuracy 0.4860\n",
            "Epoch 6 Batch 2250 Loss 1.0408 Accuracy 0.4861\n",
            "Epoch 6 Batch 2300 Loss 1.0378 Accuracy 0.4865\n",
            "Epoch 6 Batch 2350 Loss 1.0348 Accuracy 0.4867\n",
            "Epoch 6 Batch 2400 Loss 1.0324 Accuracy 0.4870\n",
            "Epoch 6 Batch 2450 Loss 1.0294 Accuracy 0.4874\n",
            "Epoch 6 Batch 2500 Loss 1.0263 Accuracy 0.4878\n",
            "Epoch 6 Batch 2550 Loss 1.0237 Accuracy 0.4882\n",
            "Epoch 6 Batch 2600 Loss 1.0208 Accuracy 0.4884\n",
            "Epoch 6 Batch 2650 Loss 1.0182 Accuracy 0.4888\n",
            "Epoch 6 Batch 2700 Loss 1.0154 Accuracy 0.4891\n",
            "Epoch 6 Batch 2750 Loss 1.0130 Accuracy 0.4894\n",
            "Epoch 6 Batch 2800 Loss 1.0112 Accuracy 0.4897\n",
            "Epoch 6 Batch 2850 Loss 1.0090 Accuracy 0.4900\n",
            "Epoch 6 Batch 2900 Loss 1.0070 Accuracy 0.4903\n",
            "Epoch 6 Batch 2950 Loss 1.0050 Accuracy 0.4905\n",
            "Epoch 6 Batch 3000 Loss 1.0035 Accuracy 0.4908\n",
            "Epoch 6 Batch 3050 Loss 1.0015 Accuracy 0.4911\n",
            "Epoch 6 Batch 3100 Loss 0.9998 Accuracy 0.4914\n",
            "Epoch 6 Batch 3150 Loss 0.9977 Accuracy 0.4915\n",
            "Epoch 6 Batch 3200 Loss 0.9957 Accuracy 0.4918\n",
            "Epoch 6 Batch 3250 Loss 0.9933 Accuracy 0.4920\n",
            "Epoch 6 Batch 3300 Loss 0.9912 Accuracy 0.4922\n",
            "Epoch 6 Batch 3350 Loss 0.9891 Accuracy 0.4924\n",
            "Epoch 6 Batch 3400 Loss 0.9872 Accuracy 0.4928\n",
            "Epoch 6 Batch 3450 Loss 0.9852 Accuracy 0.4931\n",
            "Epoch 6 Batch 3500 Loss 0.9833 Accuracy 0.4934\n",
            "Epoch 6 Batch 3550 Loss 0.9814 Accuracy 0.4937\n",
            "Epoch 6 Batch 3600 Loss 0.9797 Accuracy 0.4941\n",
            "Epoch 6 Batch 3650 Loss 0.9780 Accuracy 0.4944\n",
            "Epoch 6 Batch 3700 Loss 0.9762 Accuracy 0.4948\n",
            "Epoch 6 Batch 3750 Loss 0.9743 Accuracy 0.4951\n",
            "Epoch 6 Batch 3800 Loss 0.9726 Accuracy 0.4954\n",
            "Epoch 6 Batch 3850 Loss 0.9712 Accuracy 0.4957\n",
            "Epoch 6 Batch 3900 Loss 0.9699 Accuracy 0.4960\n",
            "Epoch 6 Batch 3950 Loss 0.9685 Accuracy 0.4963\n",
            "Epoch 6 Batch 4000 Loss 0.9673 Accuracy 0.4966\n",
            "Epoch 6 Batch 4050 Loss 0.9660 Accuracy 0.4969\n",
            "Epoch 6 Batch 4100 Loss 0.9648 Accuracy 0.4972\n",
            "Epoch 6 Batch 4150 Loss 0.9642 Accuracy 0.4973\n",
            "Epoch 6 Batch 4200 Loss 0.9642 Accuracy 0.4973\n",
            "Epoch 6 Batch 4250 Loss 0.9647 Accuracy 0.4973\n",
            "Epoch 6 Batch 4300 Loss 0.9656 Accuracy 0.4972\n",
            "Epoch 6 Batch 4350 Loss 0.9667 Accuracy 0.4971\n",
            "Epoch 6 Batch 4400 Loss 0.9678 Accuracy 0.4970\n",
            "Epoch 6 Batch 4450 Loss 0.9689 Accuracy 0.4969\n",
            "Epoch 6 Batch 4500 Loss 0.9702 Accuracy 0.4967\n",
            "Epoch 6 Batch 4550 Loss 0.9716 Accuracy 0.4965\n",
            "Epoch 6 Batch 4600 Loss 0.9731 Accuracy 0.4964\n",
            "Epoch 6 Batch 4650 Loss 0.9745 Accuracy 0.4962\n",
            "Epoch 6 Batch 4700 Loss 0.9759 Accuracy 0.4960\n",
            "Epoch 6 Batch 4750 Loss 0.9770 Accuracy 0.4959\n",
            "Epoch 6 Batch 4800 Loss 0.9784 Accuracy 0.4957\n",
            "Epoch 6 Batch 4850 Loss 0.9797 Accuracy 0.4955\n",
            "Epoch 6 Batch 4900 Loss 0.9808 Accuracy 0.4953\n",
            "Epoch 6 Batch 4950 Loss 0.9820 Accuracy 0.4951\n",
            "Epoch 6 Batch 5000 Loss 0.9833 Accuracy 0.4949\n",
            "Epoch 6 Batch 5050 Loss 0.9843 Accuracy 0.4947\n",
            "Epoch 6 Batch 5100 Loss 0.9856 Accuracy 0.4945\n",
            "Epoch 6 Batch 5150 Loss 0.9870 Accuracy 0.4942\n",
            "Epoch 6 Batch 5200 Loss 0.9882 Accuracy 0.4940\n",
            "Epoch 6 Batch 5250 Loss 0.9892 Accuracy 0.4937\n",
            "Epoch 6 Batch 5300 Loss 0.9902 Accuracy 0.4934\n",
            "Epoch 6 Batch 5350 Loss 0.9913 Accuracy 0.4931\n",
            "Epoch 6 Batch 5400 Loss 0.9922 Accuracy 0.4928\n",
            "Epoch 6 Batch 5450 Loss 0.9930 Accuracy 0.4926\n",
            "Epoch 6 Batch 5500 Loss 0.9939 Accuracy 0.4923\n",
            "Epoch 6 Batch 5550 Loss 0.9949 Accuracy 0.4921\n",
            "Epoch 6 Batch 5600 Loss 0.9960 Accuracy 0.4919\n",
            "Epoch 6 Batch 5650 Loss 0.9967 Accuracy 0.4916\n",
            "Epoch 6 Batch 5700 Loss 0.9978 Accuracy 0.4914\n",
            "Saving checkpoint for epoch 6 at ./drive/My Drive/Colab Notebooks/ModernNLP/Transformer/checkpoint/ckpt-6\n",
            "Time taken for 1 epoch: 1451.6412374973297 secs\n",
            "\n",
            "Start of epoch 7\n",
            "Epoch 7 Batch 0 Loss 0.9666 Accuracy 0.4827\n",
            "Epoch 7 Batch 50 Loss 1.1001 Accuracy 0.4696\n",
            "Epoch 7 Batch 100 Loss 1.1057 Accuracy 0.4727\n",
            "Epoch 7 Batch 150 Loss 1.0999 Accuracy 0.4757\n",
            "Epoch 7 Batch 200 Loss 1.0977 Accuracy 0.4764\n",
            "Epoch 7 Batch 250 Loss 1.0888 Accuracy 0.4766\n",
            "Epoch 7 Batch 300 Loss 1.0858 Accuracy 0.4754\n",
            "Epoch 7 Batch 350 Loss 1.0846 Accuracy 0.4753\n",
            "Epoch 7 Batch 400 Loss 1.0822 Accuracy 0.4758\n",
            "Epoch 7 Batch 450 Loss 1.0793 Accuracy 0.4761\n",
            "Epoch 7 Batch 500 Loss 1.0796 Accuracy 0.4761\n",
            "Epoch 7 Batch 550 Loss 1.0788 Accuracy 0.4763\n",
            "Epoch 7 Batch 600 Loss 1.0768 Accuracy 0.4759\n",
            "Epoch 7 Batch 650 Loss 1.0770 Accuracy 0.4763\n",
            "Epoch 7 Batch 700 Loss 1.0768 Accuracy 0.4767\n",
            "Epoch 7 Batch 750 Loss 1.0743 Accuracy 0.4774\n",
            "Epoch 7 Batch 800 Loss 1.0739 Accuracy 0.4775\n",
            "Epoch 7 Batch 850 Loss 1.0743 Accuracy 0.4775\n",
            "Epoch 7 Batch 900 Loss 1.0728 Accuracy 0.4773\n",
            "Epoch 7 Batch 950 Loss 1.0711 Accuracy 0.4773\n",
            "Epoch 7 Batch 1000 Loss 1.0686 Accuracy 0.4772\n",
            "Epoch 7 Batch 1050 Loss 1.0667 Accuracy 0.4775\n",
            "Epoch 7 Batch 1100 Loss 1.0657 Accuracy 0.4775\n",
            "Epoch 7 Batch 1150 Loss 1.0642 Accuracy 0.4777\n",
            "Epoch 7 Batch 1200 Loss 1.0623 Accuracy 0.4780\n",
            "Epoch 7 Batch 1250 Loss 1.0601 Accuracy 0.4785\n",
            "Epoch 7 Batch 1300 Loss 1.0583 Accuracy 0.4789\n",
            "Epoch 7 Batch 1350 Loss 1.0564 Accuracy 0.4792\n",
            "Epoch 7 Batch 1400 Loss 1.0534 Accuracy 0.4802\n",
            "Epoch 7 Batch 1450 Loss 1.0506 Accuracy 0.4810\n",
            "Epoch 7 Batch 1500 Loss 1.0482 Accuracy 0.4818\n",
            "Epoch 7 Batch 1550 Loss 1.0452 Accuracy 0.4825\n",
            "Epoch 7 Batch 1600 Loss 1.0429 Accuracy 0.4834\n",
            "Epoch 7 Batch 1650 Loss 1.0406 Accuracy 0.4843\n",
            "Epoch 7 Batch 1700 Loss 1.0378 Accuracy 0.4852\n",
            "Epoch 7 Batch 1750 Loss 1.0358 Accuracy 0.4858\n",
            "Epoch 7 Batch 1800 Loss 1.0332 Accuracy 0.4867\n",
            "Epoch 7 Batch 1850 Loss 1.0306 Accuracy 0.4874\n",
            "Epoch 7 Batch 1900 Loss 1.0286 Accuracy 0.4883\n",
            "Epoch 7 Batch 1950 Loss 1.0267 Accuracy 0.4890\n",
            "Epoch 7 Batch 2000 Loss 1.0256 Accuracy 0.4896\n",
            "Epoch 7 Batch 2050 Loss 1.0232 Accuracy 0.4900\n",
            "Epoch 7 Batch 2100 Loss 1.0207 Accuracy 0.4903\n",
            "Epoch 7 Batch 2150 Loss 1.0184 Accuracy 0.4906\n",
            "Epoch 7 Batch 2200 Loss 1.0152 Accuracy 0.4908\n",
            "Epoch 7 Batch 2250 Loss 1.0126 Accuracy 0.4908\n",
            "Epoch 7 Batch 2300 Loss 1.0095 Accuracy 0.4911\n",
            "Epoch 7 Batch 2350 Loss 1.0070 Accuracy 0.4914\n",
            "Epoch 7 Batch 2400 Loss 1.0044 Accuracy 0.4917\n",
            "Epoch 7 Batch 2450 Loss 1.0014 Accuracy 0.4919\n",
            "Epoch 7 Batch 2500 Loss 0.9979 Accuracy 0.4924\n",
            "Epoch 7 Batch 2550 Loss 0.9952 Accuracy 0.4927\n",
            "Epoch 7 Batch 2600 Loss 0.9927 Accuracy 0.4930\n",
            "Epoch 7 Batch 2650 Loss 0.9902 Accuracy 0.4935\n",
            "Epoch 7 Batch 2700 Loss 0.9875 Accuracy 0.4939\n",
            "Epoch 7 Batch 2750 Loss 0.9850 Accuracy 0.4941\n",
            "Epoch 7 Batch 2800 Loss 0.9825 Accuracy 0.4944\n",
            "Epoch 7 Batch 2850 Loss 0.9806 Accuracy 0.4947\n",
            "Epoch 7 Batch 2900 Loss 0.9788 Accuracy 0.4951\n",
            "Epoch 7 Batch 2950 Loss 0.9767 Accuracy 0.4954\n",
            "Epoch 7 Batch 3000 Loss 0.9749 Accuracy 0.4956\n",
            "Epoch 7 Batch 3050 Loss 0.9730 Accuracy 0.4958\n",
            "Epoch 7 Batch 3100 Loss 0.9712 Accuracy 0.4960\n",
            "Epoch 7 Batch 3150 Loss 0.9693 Accuracy 0.4962\n",
            "Epoch 7 Batch 3200 Loss 0.9678 Accuracy 0.4965\n",
            "Epoch 7 Batch 3250 Loss 0.9656 Accuracy 0.4967\n",
            "Epoch 7 Batch 3300 Loss 0.9632 Accuracy 0.4970\n",
            "Epoch 7 Batch 3350 Loss 0.9612 Accuracy 0.4974\n",
            "Epoch 7 Batch 3400 Loss 0.9595 Accuracy 0.4976\n",
            "Epoch 7 Batch 3450 Loss 0.9578 Accuracy 0.4978\n",
            "Epoch 7 Batch 3500 Loss 0.9562 Accuracy 0.4981\n",
            "Epoch 7 Batch 3550 Loss 0.9544 Accuracy 0.4983\n",
            "Epoch 7 Batch 3600 Loss 0.9527 Accuracy 0.4987\n",
            "Epoch 7 Batch 3650 Loss 0.9511 Accuracy 0.4990\n",
            "Epoch 7 Batch 3700 Loss 0.9491 Accuracy 0.4994\n",
            "Epoch 7 Batch 3750 Loss 0.9477 Accuracy 0.4997\n",
            "Epoch 7 Batch 3800 Loss 0.9461 Accuracy 0.5000\n",
            "Epoch 7 Batch 3850 Loss 0.9445 Accuracy 0.5003\n",
            "Epoch 7 Batch 3900 Loss 0.9432 Accuracy 0.5007\n",
            "Epoch 7 Batch 3950 Loss 0.9417 Accuracy 0.5010\n",
            "Epoch 7 Batch 4000 Loss 0.9403 Accuracy 0.5013\n",
            "Epoch 7 Batch 4050 Loss 0.9390 Accuracy 0.5017\n",
            "Epoch 7 Batch 4100 Loss 0.9382 Accuracy 0.5018\n",
            "Epoch 7 Batch 4150 Loss 0.9378 Accuracy 0.5020\n",
            "Epoch 7 Batch 4200 Loss 0.9378 Accuracy 0.5020\n",
            "Epoch 7 Batch 4250 Loss 0.9381 Accuracy 0.5019\n",
            "Epoch 7 Batch 4300 Loss 0.9391 Accuracy 0.5019\n",
            "Epoch 7 Batch 4350 Loss 0.9399 Accuracy 0.5018\n",
            "Epoch 7 Batch 4400 Loss 0.9410 Accuracy 0.5016\n",
            "Epoch 7 Batch 4450 Loss 0.9422 Accuracy 0.5015\n",
            "Epoch 7 Batch 4500 Loss 0.9438 Accuracy 0.5012\n",
            "Epoch 7 Batch 4550 Loss 0.9449 Accuracy 0.5010\n",
            "Epoch 7 Batch 4600 Loss 0.9464 Accuracy 0.5009\n",
            "Epoch 7 Batch 4650 Loss 0.9476 Accuracy 0.5007\n",
            "Epoch 7 Batch 4700 Loss 0.9490 Accuracy 0.5005\n",
            "Epoch 7 Batch 4750 Loss 0.9503 Accuracy 0.5003\n",
            "Epoch 7 Batch 4800 Loss 0.9514 Accuracy 0.5001\n",
            "Epoch 7 Batch 4850 Loss 0.9525 Accuracy 0.4999\n",
            "Epoch 7 Batch 4900 Loss 0.9537 Accuracy 0.4997\n",
            "Epoch 7 Batch 4950 Loss 0.9547 Accuracy 0.4995\n",
            "Epoch 7 Batch 5000 Loss 0.9560 Accuracy 0.4993\n",
            "Epoch 7 Batch 5050 Loss 0.9572 Accuracy 0.4991\n",
            "Epoch 7 Batch 5100 Loss 0.9585 Accuracy 0.4989\n",
            "Epoch 7 Batch 5150 Loss 0.9598 Accuracy 0.4986\n",
            "Epoch 7 Batch 5200 Loss 0.9610 Accuracy 0.4983\n",
            "Epoch 7 Batch 5250 Loss 0.9624 Accuracy 0.4981\n",
            "Epoch 7 Batch 5300 Loss 0.9635 Accuracy 0.4978\n",
            "Epoch 7 Batch 5350 Loss 0.9648 Accuracy 0.4975\n",
            "Epoch 7 Batch 5400 Loss 0.9659 Accuracy 0.4972\n",
            "Epoch 7 Batch 5450 Loss 0.9669 Accuracy 0.4970\n",
            "Epoch 7 Batch 5500 Loss 0.9677 Accuracy 0.4967\n",
            "Epoch 7 Batch 5550 Loss 0.9686 Accuracy 0.4965\n",
            "Epoch 7 Batch 5600 Loss 0.9695 Accuracy 0.4962\n",
            "Epoch 7 Batch 5650 Loss 0.9705 Accuracy 0.4960\n",
            "Epoch 7 Batch 5700 Loss 0.9714 Accuracy 0.4957\n",
            "Saving checkpoint for epoch 7 at ./drive/My Drive/Colab Notebooks/ModernNLP/Transformer/checkpoint/ckpt-7\n",
            "Time taken for 1 epoch: 1456.4340016841888 secs\n",
            "\n",
            "Start of epoch 8\n",
            "Epoch 8 Batch 0 Loss 1.0273 Accuracy 0.4803\n",
            "Epoch 8 Batch 50 Loss 1.0812 Accuracy 0.4798\n",
            "Epoch 8 Batch 100 Loss 1.0724 Accuracy 0.4794\n",
            "Epoch 8 Batch 150 Loss 1.0685 Accuracy 0.4796\n",
            "Epoch 8 Batch 200 Loss 1.0678 Accuracy 0.4789\n",
            "Epoch 8 Batch 250 Loss 1.0622 Accuracy 0.4802\n",
            "Epoch 8 Batch 300 Loss 1.0636 Accuracy 0.4797\n",
            "Epoch 8 Batch 350 Loss 1.0645 Accuracy 0.4791\n",
            "Epoch 8 Batch 400 Loss 1.0647 Accuracy 0.4796\n",
            "Epoch 8 Batch 450 Loss 1.0631 Accuracy 0.4799\n",
            "Epoch 8 Batch 500 Loss 1.0617 Accuracy 0.4803\n",
            "Epoch 8 Batch 550 Loss 1.0578 Accuracy 0.4804\n",
            "Epoch 8 Batch 600 Loss 1.0540 Accuracy 0.4804\n",
            "Epoch 8 Batch 650 Loss 1.0517 Accuracy 0.4803\n",
            "Epoch 8 Batch 700 Loss 1.0509 Accuracy 0.4806\n",
            "Epoch 8 Batch 750 Loss 1.0498 Accuracy 0.4809\n",
            "Epoch 8 Batch 800 Loss 1.0489 Accuracy 0.4813\n",
            "Epoch 8 Batch 850 Loss 1.0487 Accuracy 0.4812\n",
            "Epoch 8 Batch 900 Loss 1.0479 Accuracy 0.4811\n",
            "Epoch 8 Batch 950 Loss 1.0461 Accuracy 0.4813\n",
            "Epoch 8 Batch 1000 Loss 1.0444 Accuracy 0.4815\n",
            "Epoch 8 Batch 1050 Loss 1.0433 Accuracy 0.4818\n",
            "Epoch 8 Batch 1100 Loss 1.0417 Accuracy 0.4818\n",
            "Epoch 8 Batch 1150 Loss 1.0405 Accuracy 0.4821\n",
            "Epoch 8 Batch 1200 Loss 1.0388 Accuracy 0.4823\n",
            "Epoch 8 Batch 1250 Loss 1.0369 Accuracy 0.4825\n",
            "Epoch 8 Batch 1300 Loss 1.0349 Accuracy 0.4829\n",
            "Epoch 8 Batch 1350 Loss 1.0329 Accuracy 0.4835\n",
            "Epoch 8 Batch 1400 Loss 1.0305 Accuracy 0.4842\n",
            "Epoch 8 Batch 1450 Loss 1.0284 Accuracy 0.4850\n",
            "Epoch 8 Batch 1500 Loss 1.0268 Accuracy 0.4857\n",
            "Epoch 8 Batch 1550 Loss 1.0236 Accuracy 0.4867\n",
            "Epoch 8 Batch 1600 Loss 1.0210 Accuracy 0.4873\n",
            "Epoch 8 Batch 1650 Loss 1.0185 Accuracy 0.4881\n",
            "Epoch 8 Batch 1700 Loss 1.0162 Accuracy 0.4890\n",
            "Epoch 8 Batch 1750 Loss 1.0141 Accuracy 0.4896\n",
            "Epoch 8 Batch 1800 Loss 1.0121 Accuracy 0.4903\n",
            "Epoch 8 Batch 1850 Loss 1.0104 Accuracy 0.4910\n",
            "Epoch 8 Batch 1900 Loss 1.0078 Accuracy 0.4918\n",
            "Epoch 8 Batch 1950 Loss 1.0047 Accuracy 0.4926\n",
            "Epoch 8 Batch 2000 Loss 1.0032 Accuracy 0.4934\n",
            "Epoch 8 Batch 2050 Loss 1.0007 Accuracy 0.4938\n",
            "Epoch 8 Batch 2100 Loss 0.9983 Accuracy 0.4942\n",
            "Epoch 8 Batch 2150 Loss 0.9955 Accuracy 0.4944\n",
            "Epoch 8 Batch 2200 Loss 0.9926 Accuracy 0.4946\n",
            "Epoch 8 Batch 2250 Loss 0.9901 Accuracy 0.4947\n",
            "Epoch 8 Batch 2300 Loss 0.9870 Accuracy 0.4949\n",
            "Epoch 8 Batch 2350 Loss 0.9848 Accuracy 0.4950\n",
            "Epoch 8 Batch 2400 Loss 0.9819 Accuracy 0.4951\n",
            "Epoch 8 Batch 2450 Loss 0.9788 Accuracy 0.4955\n",
            "Epoch 8 Batch 2500 Loss 0.9762 Accuracy 0.4958\n",
            "Epoch 8 Batch 2550 Loss 0.9740 Accuracy 0.4961\n",
            "Epoch 8 Batch 2600 Loss 0.9704 Accuracy 0.4965\n",
            "Epoch 8 Batch 2650 Loss 0.9675 Accuracy 0.4969\n",
            "Epoch 8 Batch 2700 Loss 0.9653 Accuracy 0.4972\n",
            "Epoch 8 Batch 2750 Loss 0.9633 Accuracy 0.4975\n",
            "Epoch 8 Batch 2800 Loss 0.9612 Accuracy 0.4978\n",
            "Epoch 8 Batch 2850 Loss 0.9587 Accuracy 0.4982\n",
            "Epoch 8 Batch 2900 Loss 0.9567 Accuracy 0.4985\n",
            "Epoch 8 Batch 2950 Loss 0.9547 Accuracy 0.4988\n",
            "Epoch 8 Batch 3000 Loss 0.9528 Accuracy 0.4990\n",
            "Epoch 8 Batch 3050 Loss 0.9507 Accuracy 0.4992\n",
            "Epoch 8 Batch 3100 Loss 0.9490 Accuracy 0.4996\n",
            "Epoch 8 Batch 3150 Loss 0.9474 Accuracy 0.4998\n",
            "Epoch 8 Batch 3200 Loss 0.9454 Accuracy 0.5000\n",
            "Epoch 8 Batch 3250 Loss 0.9433 Accuracy 0.5002\n",
            "Epoch 8 Batch 3300 Loss 0.9413 Accuracy 0.5005\n",
            "Epoch 8 Batch 3350 Loss 0.9396 Accuracy 0.5007\n",
            "Epoch 8 Batch 3400 Loss 0.9376 Accuracy 0.5010\n",
            "Epoch 8 Batch 3450 Loss 0.9354 Accuracy 0.5013\n",
            "Epoch 8 Batch 3500 Loss 0.9338 Accuracy 0.5016\n",
            "Epoch 8 Batch 3550 Loss 0.9321 Accuracy 0.5020\n",
            "Epoch 8 Batch 3600 Loss 0.9303 Accuracy 0.5023\n",
            "Epoch 8 Batch 3650 Loss 0.9285 Accuracy 0.5026\n",
            "Epoch 8 Batch 3700 Loss 0.9268 Accuracy 0.5029\n",
            "Epoch 8 Batch 3750 Loss 0.9251 Accuracy 0.5033\n",
            "Epoch 8 Batch 3800 Loss 0.9234 Accuracy 0.5036\n",
            "Epoch 8 Batch 3850 Loss 0.9224 Accuracy 0.5040\n",
            "Epoch 8 Batch 3900 Loss 0.9212 Accuracy 0.5043\n",
            "Epoch 8 Batch 3950 Loss 0.9199 Accuracy 0.5046\n",
            "Epoch 8 Batch 4000 Loss 0.9188 Accuracy 0.5049\n",
            "Epoch 8 Batch 4050 Loss 0.9174 Accuracy 0.5052\n",
            "Epoch 8 Batch 4100 Loss 0.9163 Accuracy 0.5054\n",
            "Epoch 8 Batch 4150 Loss 0.9161 Accuracy 0.5054\n",
            "Epoch 8 Batch 4200 Loss 0.9163 Accuracy 0.5054\n",
            "Epoch 8 Batch 4250 Loss 0.9166 Accuracy 0.5053\n",
            "Epoch 8 Batch 4300 Loss 0.9173 Accuracy 0.5052\n",
            "Epoch 8 Batch 4350 Loss 0.9185 Accuracy 0.5051\n",
            "Epoch 8 Batch 4400 Loss 0.9197 Accuracy 0.5050\n",
            "Epoch 8 Batch 4450 Loss 0.9206 Accuracy 0.5048\n",
            "Epoch 8 Batch 4500 Loss 0.9218 Accuracy 0.5045\n",
            "Epoch 8 Batch 4550 Loss 0.9233 Accuracy 0.5043\n",
            "Epoch 8 Batch 4600 Loss 0.9248 Accuracy 0.5042\n",
            "Epoch 8 Batch 4650 Loss 0.9262 Accuracy 0.5040\n",
            "Epoch 8 Batch 4700 Loss 0.9277 Accuracy 0.5038\n",
            "Epoch 8 Batch 4750 Loss 0.9290 Accuracy 0.5037\n",
            "Epoch 8 Batch 4800 Loss 0.9304 Accuracy 0.5034\n",
            "Epoch 8 Batch 4850 Loss 0.9315 Accuracy 0.5033\n",
            "Epoch 8 Batch 4900 Loss 0.9328 Accuracy 0.5031\n",
            "Epoch 8 Batch 4950 Loss 0.9340 Accuracy 0.5030\n",
            "Epoch 8 Batch 5000 Loss 0.9353 Accuracy 0.5028\n",
            "Epoch 8 Batch 5050 Loss 0.9367 Accuracy 0.5026\n",
            "Epoch 8 Batch 5100 Loss 0.9381 Accuracy 0.5023\n",
            "Epoch 8 Batch 5150 Loss 0.9393 Accuracy 0.5021\n",
            "Epoch 8 Batch 5200 Loss 0.9406 Accuracy 0.5018\n",
            "Epoch 8 Batch 5250 Loss 0.9418 Accuracy 0.5015\n",
            "Epoch 8 Batch 5300 Loss 0.9429 Accuracy 0.5012\n",
            "Epoch 8 Batch 5350 Loss 0.9441 Accuracy 0.5009\n",
            "Epoch 8 Batch 5400 Loss 0.9451 Accuracy 0.5006\n",
            "Epoch 8 Batch 5450 Loss 0.9459 Accuracy 0.5004\n",
            "Epoch 8 Batch 5500 Loss 0.9470 Accuracy 0.5001\n",
            "Epoch 8 Batch 5550 Loss 0.9479 Accuracy 0.4999\n",
            "Epoch 8 Batch 5600 Loss 0.9488 Accuracy 0.4997\n",
            "Epoch 8 Batch 5650 Loss 0.9496 Accuracy 0.4994\n",
            "Epoch 8 Batch 5700 Loss 0.9504 Accuracy 0.4992\n",
            "Saving checkpoint for epoch 8 at ./drive/My Drive/Colab Notebooks/ModernNLP/Transformer/checkpoint/ckpt-8\n",
            "Time taken for 1 epoch: 1443.1470313072205 secs\n",
            "\n",
            "Start of epoch 9\n",
            "Epoch 9 Batch 0 Loss 1.0687 Accuracy 0.4819\n",
            "Epoch 9 Batch 50 Loss 1.0598 Accuracy 0.4841\n",
            "Epoch 9 Batch 100 Loss 1.0441 Accuracy 0.4811\n",
            "Epoch 9 Batch 150 Loss 1.0393 Accuracy 0.4822\n",
            "Epoch 9 Batch 200 Loss 1.0401 Accuracy 0.4831\n",
            "Epoch 9 Batch 250 Loss 1.0422 Accuracy 0.4825\n",
            "Epoch 9 Batch 300 Loss 1.0423 Accuracy 0.4832\n",
            "Epoch 9 Batch 350 Loss 1.0412 Accuracy 0.4828\n",
            "Epoch 9 Batch 400 Loss 1.0392 Accuracy 0.4826\n",
            "Epoch 9 Batch 450 Loss 1.0380 Accuracy 0.4830\n",
            "Epoch 9 Batch 500 Loss 1.0376 Accuracy 0.4827\n",
            "Epoch 9 Batch 550 Loss 1.0379 Accuracy 0.4821\n",
            "Epoch 9 Batch 600 Loss 1.0380 Accuracy 0.4825\n",
            "Epoch 9 Batch 650 Loss 1.0354 Accuracy 0.4826\n",
            "Epoch 9 Batch 700 Loss 1.0335 Accuracy 0.4831\n",
            "Epoch 9 Batch 750 Loss 1.0327 Accuracy 0.4832\n",
            "Epoch 9 Batch 800 Loss 1.0318 Accuracy 0.4835\n",
            "Epoch 9 Batch 850 Loss 1.0312 Accuracy 0.4835\n",
            "Epoch 9 Batch 900 Loss 1.0299 Accuracy 0.4838\n",
            "Epoch 9 Batch 950 Loss 1.0287 Accuracy 0.4838\n",
            "Epoch 9 Batch 1000 Loss 1.0265 Accuracy 0.4843\n",
            "Epoch 9 Batch 1050 Loss 1.0247 Accuracy 0.4844\n",
            "Epoch 9 Batch 1100 Loss 1.0228 Accuracy 0.4846\n",
            "Epoch 9 Batch 1150 Loss 1.0220 Accuracy 0.4848\n",
            "Epoch 9 Batch 1200 Loss 1.0197 Accuracy 0.4851\n",
            "Epoch 9 Batch 1250 Loss 1.0175 Accuracy 0.4855\n",
            "Epoch 9 Batch 1300 Loss 1.0153 Accuracy 0.4859\n",
            "Epoch 9 Batch 1350 Loss 1.0131 Accuracy 0.4866\n",
            "Epoch 9 Batch 1400 Loss 1.0116 Accuracy 0.4872\n",
            "Epoch 9 Batch 1450 Loss 1.0090 Accuracy 0.4880\n",
            "Epoch 9 Batch 1500 Loss 1.0069 Accuracy 0.4888\n",
            "Epoch 9 Batch 1550 Loss 1.0039 Accuracy 0.4896\n",
            "Epoch 9 Batch 1600 Loss 1.0013 Accuracy 0.4906\n",
            "Epoch 9 Batch 1650 Loss 0.9989 Accuracy 0.4914\n",
            "Epoch 9 Batch 1700 Loss 0.9968 Accuracy 0.4921\n",
            "Epoch 9 Batch 1750 Loss 0.9947 Accuracy 0.4928\n",
            "Epoch 9 Batch 1800 Loss 0.9930 Accuracy 0.4935\n",
            "Epoch 9 Batch 1850 Loss 0.9910 Accuracy 0.4944\n",
            "Epoch 9 Batch 1900 Loss 0.9888 Accuracy 0.4951\n",
            "Epoch 9 Batch 1950 Loss 0.9865 Accuracy 0.4957\n",
            "Epoch 9 Batch 2000 Loss 0.9847 Accuracy 0.4962\n",
            "Epoch 9 Batch 2050 Loss 0.9829 Accuracy 0.4968\n",
            "Epoch 9 Batch 2100 Loss 0.9802 Accuracy 0.4972\n",
            "Epoch 9 Batch 2150 Loss 0.9783 Accuracy 0.4975\n",
            "Epoch 9 Batch 2200 Loss 0.9753 Accuracy 0.4977\n",
            "Epoch 9 Batch 2250 Loss 0.9720 Accuracy 0.4980\n",
            "Epoch 9 Batch 2300 Loss 0.9690 Accuracy 0.4980\n",
            "Epoch 9 Batch 2350 Loss 0.9658 Accuracy 0.4983\n",
            "Epoch 9 Batch 2400 Loss 0.9630 Accuracy 0.4987\n",
            "Epoch 9 Batch 2450 Loss 0.9602 Accuracy 0.4989\n",
            "Epoch 9 Batch 2500 Loss 0.9574 Accuracy 0.4992\n",
            "Epoch 9 Batch 2550 Loss 0.9550 Accuracy 0.4994\n",
            "Epoch 9 Batch 2600 Loss 0.9522 Accuracy 0.4998\n",
            "Epoch 9 Batch 2650 Loss 0.9501 Accuracy 0.5002\n",
            "Epoch 9 Batch 2700 Loss 0.9476 Accuracy 0.5006\n",
            "Epoch 9 Batch 2750 Loss 0.9450 Accuracy 0.5009\n",
            "Epoch 9 Batch 2800 Loss 0.9427 Accuracy 0.5011\n",
            "Epoch 9 Batch 2850 Loss 0.9408 Accuracy 0.5014\n",
            "Epoch 9 Batch 2900 Loss 0.9386 Accuracy 0.5016\n",
            "Epoch 9 Batch 2950 Loss 0.9367 Accuracy 0.5019\n",
            "Epoch 9 Batch 3000 Loss 0.9346 Accuracy 0.5021\n",
            "Epoch 9 Batch 3050 Loss 0.9328 Accuracy 0.5023\n",
            "Epoch 9 Batch 3100 Loss 0.9310 Accuracy 0.5026\n",
            "Epoch 9 Batch 3150 Loss 0.9294 Accuracy 0.5028\n",
            "Epoch 9 Batch 3200 Loss 0.9274 Accuracy 0.5030\n",
            "Epoch 9 Batch 3250 Loss 0.9255 Accuracy 0.5033\n",
            "Epoch 9 Batch 3300 Loss 0.9236 Accuracy 0.5035\n",
            "Epoch 9 Batch 3350 Loss 0.9214 Accuracy 0.5038\n",
            "Epoch 9 Batch 3400 Loss 0.9198 Accuracy 0.5041\n",
            "Epoch 9 Batch 3450 Loss 0.9180 Accuracy 0.5044\n",
            "Epoch 9 Batch 3500 Loss 0.9165 Accuracy 0.5047\n",
            "Epoch 9 Batch 3550 Loss 0.9147 Accuracy 0.5050\n",
            "Epoch 9 Batch 3600 Loss 0.9130 Accuracy 0.5053\n",
            "Epoch 9 Batch 3650 Loss 0.9113 Accuracy 0.5056\n",
            "Epoch 9 Batch 3700 Loss 0.9097 Accuracy 0.5060\n",
            "Epoch 9 Batch 3750 Loss 0.9078 Accuracy 0.5062\n",
            "Epoch 9 Batch 3800 Loss 0.9062 Accuracy 0.5066\n",
            "Epoch 9 Batch 3850 Loss 0.9045 Accuracy 0.5069\n",
            "Epoch 9 Batch 3900 Loss 0.9034 Accuracy 0.5073\n",
            "Epoch 9 Batch 3950 Loss 0.9020 Accuracy 0.5076\n",
            "Epoch 9 Batch 4000 Loss 0.9005 Accuracy 0.5079\n",
            "Epoch 9 Batch 4050 Loss 0.8995 Accuracy 0.5082\n",
            "Epoch 9 Batch 4100 Loss 0.8984 Accuracy 0.5084\n",
            "Epoch 9 Batch 4150 Loss 0.8982 Accuracy 0.5085\n",
            "Epoch 9 Batch 4200 Loss 0.8983 Accuracy 0.5085\n",
            "Epoch 9 Batch 4250 Loss 0.8992 Accuracy 0.5085\n",
            "Epoch 9 Batch 4300 Loss 0.9000 Accuracy 0.5083\n",
            "Epoch 9 Batch 4350 Loss 0.9009 Accuracy 0.5082\n",
            "Epoch 9 Batch 4400 Loss 0.9020 Accuracy 0.5080\n",
            "Epoch 9 Batch 4450 Loss 0.9033 Accuracy 0.5078\n",
            "Epoch 9 Batch 4500 Loss 0.9046 Accuracy 0.5076\n",
            "Epoch 9 Batch 4550 Loss 0.9059 Accuracy 0.5074\n",
            "Epoch 9 Batch 4600 Loss 0.9073 Accuracy 0.5071\n",
            "Epoch 9 Batch 4650 Loss 0.9084 Accuracy 0.5069\n",
            "Epoch 9 Batch 4700 Loss 0.9098 Accuracy 0.5067\n",
            "Epoch 9 Batch 4750 Loss 0.9110 Accuracy 0.5066\n",
            "Epoch 9 Batch 4800 Loss 0.9124 Accuracy 0.5064\n",
            "Epoch 9 Batch 4850 Loss 0.9137 Accuracy 0.5062\n",
            "Epoch 9 Batch 4900 Loss 0.9149 Accuracy 0.5061\n",
            "Epoch 9 Batch 4950 Loss 0.9162 Accuracy 0.5059\n",
            "Epoch 9 Batch 5000 Loss 0.9173 Accuracy 0.5057\n",
            "Epoch 9 Batch 5050 Loss 0.9187 Accuracy 0.5055\n",
            "Epoch 9 Batch 5100 Loss 0.9198 Accuracy 0.5053\n",
            "Epoch 9 Batch 5150 Loss 0.9211 Accuracy 0.5050\n",
            "Epoch 9 Batch 5200 Loss 0.9223 Accuracy 0.5047\n",
            "Epoch 9 Batch 5250 Loss 0.9235 Accuracy 0.5044\n",
            "Epoch 9 Batch 5300 Loss 0.9248 Accuracy 0.5041\n",
            "Epoch 9 Batch 5350 Loss 0.9260 Accuracy 0.5038\n",
            "Epoch 9 Batch 5400 Loss 0.9270 Accuracy 0.5035\n",
            "Epoch 9 Batch 5450 Loss 0.9281 Accuracy 0.5032\n",
            "Epoch 9 Batch 5500 Loss 0.9291 Accuracy 0.5029\n",
            "Epoch 9 Batch 5550 Loss 0.9301 Accuracy 0.5027\n",
            "Epoch 9 Batch 5600 Loss 0.9313 Accuracy 0.5025\n",
            "Epoch 9 Batch 5650 Loss 0.9323 Accuracy 0.5022\n",
            "Epoch 9 Batch 5700 Loss 0.9331 Accuracy 0.5020\n",
            "Saving checkpoint for epoch 9 at ./drive/My Drive/Colab Notebooks/ModernNLP/Transformer/checkpoint/ckpt-9\n",
            "Time taken for 1 epoch: 1445.613914489746 secs\n",
            "\n",
            "Start of epoch 10\n",
            "Epoch 10 Batch 0 Loss 1.0809 Accuracy 0.5008\n",
            "Epoch 10 Batch 50 Loss 1.0558 Accuracy 0.4864\n",
            "Epoch 10 Batch 100 Loss 1.0416 Accuracy 0.4884\n",
            "Epoch 10 Batch 150 Loss 1.0346 Accuracy 0.4881\n",
            "Epoch 10 Batch 200 Loss 1.0325 Accuracy 0.4875\n",
            "Epoch 10 Batch 250 Loss 1.0290 Accuracy 0.4879\n",
            "Epoch 10 Batch 300 Loss 1.0253 Accuracy 0.4873\n",
            "Epoch 10 Batch 350 Loss 1.0220 Accuracy 0.4867\n",
            "Epoch 10 Batch 400 Loss 1.0213 Accuracy 0.4861\n",
            "Epoch 10 Batch 450 Loss 1.0223 Accuracy 0.4860\n",
            "Epoch 10 Batch 500 Loss 1.0197 Accuracy 0.4860\n",
            "Epoch 10 Batch 550 Loss 1.0196 Accuracy 0.4858\n",
            "Epoch 10 Batch 600 Loss 1.0180 Accuracy 0.4855\n",
            "Epoch 10 Batch 650 Loss 1.0194 Accuracy 0.4859\n",
            "Epoch 10 Batch 700 Loss 1.0177 Accuracy 0.4864\n",
            "Epoch 10 Batch 750 Loss 1.0167 Accuracy 0.4866\n",
            "Epoch 10 Batch 800 Loss 1.0161 Accuracy 0.4867\n",
            "Epoch 10 Batch 850 Loss 1.0140 Accuracy 0.4869\n",
            "Epoch 10 Batch 900 Loss 1.0131 Accuracy 0.4872\n",
            "Epoch 10 Batch 950 Loss 1.0119 Accuracy 0.4871\n",
            "Epoch 10 Batch 1000 Loss 1.0100 Accuracy 0.4870\n",
            "Epoch 10 Batch 1050 Loss 1.0098 Accuracy 0.4873\n",
            "Epoch 10 Batch 1100 Loss 1.0085 Accuracy 0.4875\n",
            "Epoch 10 Batch 1150 Loss 1.0071 Accuracy 0.4876\n",
            "Epoch 10 Batch 1200 Loss 1.0053 Accuracy 0.4877\n",
            "Epoch 10 Batch 1250 Loss 1.0040 Accuracy 0.4880\n",
            "Epoch 10 Batch 1300 Loss 1.0012 Accuracy 0.4886\n",
            "Epoch 10 Batch 1350 Loss 0.9988 Accuracy 0.4890\n",
            "Epoch 10 Batch 1400 Loss 0.9965 Accuracy 0.4898\n",
            "Epoch 10 Batch 1450 Loss 0.9938 Accuracy 0.4905\n",
            "Epoch 10 Batch 1500 Loss 0.9913 Accuracy 0.4915\n",
            "Epoch 10 Batch 1550 Loss 0.9883 Accuracy 0.4923\n",
            "Epoch 10 Batch 1600 Loss 0.9859 Accuracy 0.4931\n",
            "Epoch 10 Batch 1650 Loss 0.9833 Accuracy 0.4938\n",
            "Epoch 10 Batch 1700 Loss 0.9813 Accuracy 0.4947\n",
            "Epoch 10 Batch 1750 Loss 0.9795 Accuracy 0.4955\n",
            "Epoch 10 Batch 1800 Loss 0.9776 Accuracy 0.4963\n",
            "Epoch 10 Batch 1850 Loss 0.9753 Accuracy 0.4971\n",
            "Epoch 10 Batch 1900 Loss 0.9730 Accuracy 0.4978\n",
            "Epoch 10 Batch 1950 Loss 0.9713 Accuracy 0.4985\n",
            "Epoch 10 Batch 2000 Loss 0.9693 Accuracy 0.4992\n",
            "Epoch 10 Batch 2050 Loss 0.9670 Accuracy 0.4996\n",
            "Epoch 10 Batch 2100 Loss 0.9645 Accuracy 0.5000\n",
            "Epoch 10 Batch 2150 Loss 0.9611 Accuracy 0.5004\n",
            "Epoch 10 Batch 2200 Loss 0.9590 Accuracy 0.5005\n",
            "Epoch 10 Batch 2250 Loss 0.9558 Accuracy 0.5006\n",
            "Epoch 10 Batch 2300 Loss 0.9529 Accuracy 0.5008\n",
            "Epoch 10 Batch 2350 Loss 0.9504 Accuracy 0.5010\n",
            "Epoch 10 Batch 2400 Loss 0.9474 Accuracy 0.5012\n",
            "Epoch 10 Batch 2450 Loss 0.9448 Accuracy 0.5015\n",
            "Epoch 10 Batch 2500 Loss 0.9420 Accuracy 0.5018\n",
            "Epoch 10 Batch 2550 Loss 0.9391 Accuracy 0.5021\n",
            "Epoch 10 Batch 2600 Loss 0.9364 Accuracy 0.5024\n",
            "Epoch 10 Batch 2650 Loss 0.9336 Accuracy 0.5028\n",
            "Epoch 10 Batch 2700 Loss 0.9313 Accuracy 0.5031\n",
            "Epoch 10 Batch 2750 Loss 0.9289 Accuracy 0.5033\n",
            "Epoch 10 Batch 2800 Loss 0.9268 Accuracy 0.5036\n",
            "Epoch 10 Batch 2850 Loss 0.9246 Accuracy 0.5038\n",
            "Epoch 10 Batch 2900 Loss 0.9224 Accuracy 0.5041\n",
            "Epoch 10 Batch 2950 Loss 0.9205 Accuracy 0.5044\n",
            "Epoch 10 Batch 3000 Loss 0.9185 Accuracy 0.5047\n",
            "Epoch 10 Batch 3050 Loss 0.9167 Accuracy 0.5050\n",
            "Epoch 10 Batch 3100 Loss 0.9149 Accuracy 0.5052\n",
            "Epoch 10 Batch 3150 Loss 0.9132 Accuracy 0.5054\n",
            "Epoch 10 Batch 3200 Loss 0.9114 Accuracy 0.5057\n",
            "Epoch 10 Batch 3250 Loss 0.9099 Accuracy 0.5059\n",
            "Epoch 10 Batch 3300 Loss 0.9081 Accuracy 0.5061\n",
            "Epoch 10 Batch 3350 Loss 0.9064 Accuracy 0.5063\n",
            "Epoch 10 Batch 3400 Loss 0.9043 Accuracy 0.5066\n",
            "Epoch 10 Batch 3450 Loss 0.9023 Accuracy 0.5069\n",
            "Epoch 10 Batch 3500 Loss 0.9005 Accuracy 0.5073\n",
            "Epoch 10 Batch 3550 Loss 0.8991 Accuracy 0.5076\n",
            "Epoch 10 Batch 3600 Loss 0.8975 Accuracy 0.5079\n",
            "Epoch 10 Batch 3650 Loss 0.8958 Accuracy 0.5082\n",
            "Epoch 10 Batch 3700 Loss 0.8941 Accuracy 0.5086\n",
            "Epoch 10 Batch 3750 Loss 0.8926 Accuracy 0.5089\n",
            "Epoch 10 Batch 3800 Loss 0.8913 Accuracy 0.5092\n",
            "Epoch 10 Batch 3850 Loss 0.8900 Accuracy 0.5095\n",
            "Epoch 10 Batch 3900 Loss 0.8889 Accuracy 0.5098\n",
            "Epoch 10 Batch 3950 Loss 0.8877 Accuracy 0.5101\n",
            "Epoch 10 Batch 4000 Loss 0.8865 Accuracy 0.5105\n",
            "Epoch 10 Batch 4050 Loss 0.8851 Accuracy 0.5108\n",
            "Epoch 10 Batch 4100 Loss 0.8840 Accuracy 0.5110\n",
            "Epoch 10 Batch 4150 Loss 0.8836 Accuracy 0.5111\n",
            "Epoch 10 Batch 4200 Loss 0.8836 Accuracy 0.5112\n",
            "Epoch 10 Batch 4250 Loss 0.8842 Accuracy 0.5111\n",
            "Epoch 10 Batch 4300 Loss 0.8847 Accuracy 0.5110\n",
            "Epoch 10 Batch 4350 Loss 0.8857 Accuracy 0.5108\n",
            "Epoch 10 Batch 4400 Loss 0.8870 Accuracy 0.5106\n",
            "Epoch 10 Batch 4450 Loss 0.8880 Accuracy 0.5104\n",
            "Epoch 10 Batch 4500 Loss 0.8891 Accuracy 0.5102\n",
            "Epoch 10 Batch 4550 Loss 0.8904 Accuracy 0.5101\n",
            "Epoch 10 Batch 4600 Loss 0.8917 Accuracy 0.5099\n",
            "Epoch 10 Batch 4650 Loss 0.8931 Accuracy 0.5096\n",
            "Epoch 10 Batch 4700 Loss 0.8944 Accuracy 0.5094\n",
            "Epoch 10 Batch 4750 Loss 0.8960 Accuracy 0.5092\n",
            "Epoch 10 Batch 4800 Loss 0.8974 Accuracy 0.5089\n",
            "Epoch 10 Batch 4850 Loss 0.8988 Accuracy 0.5087\n",
            "Epoch 10 Batch 4900 Loss 0.9001 Accuracy 0.5086\n",
            "Epoch 10 Batch 4950 Loss 0.9013 Accuracy 0.5084\n",
            "Epoch 10 Batch 5000 Loss 0.9025 Accuracy 0.5081\n",
            "Epoch 10 Batch 5050 Loss 0.9039 Accuracy 0.5079\n",
            "Epoch 10 Batch 5100 Loss 0.9052 Accuracy 0.5077\n",
            "Epoch 10 Batch 5150 Loss 0.9066 Accuracy 0.5074\n",
            "Epoch 10 Batch 5200 Loss 0.9081 Accuracy 0.5072\n",
            "Epoch 10 Batch 5250 Loss 0.9092 Accuracy 0.5068\n",
            "Epoch 10 Batch 5300 Loss 0.9104 Accuracy 0.5065\n",
            "Epoch 10 Batch 5350 Loss 0.9115 Accuracy 0.5063\n",
            "Epoch 10 Batch 5400 Loss 0.9127 Accuracy 0.5060\n",
            "Epoch 10 Batch 5450 Loss 0.9139 Accuracy 0.5057\n",
            "Epoch 10 Batch 5500 Loss 0.9148 Accuracy 0.5054\n",
            "Epoch 10 Batch 5550 Loss 0.9159 Accuracy 0.5051\n",
            "Epoch 10 Batch 5600 Loss 0.9167 Accuracy 0.5050\n",
            "Epoch 10 Batch 5650 Loss 0.9176 Accuracy 0.5048\n",
            "Epoch 10 Batch 5700 Loss 0.9186 Accuracy 0.5045\n",
            "Saving checkpoint for epoch 10 at ./drive/My Drive/Colab Notebooks/ModernNLP/Transformer/checkpoint/ckpt-10\n",
            "Time taken for 1 epoch: 1448.8727164268494 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBksUh3u-ITQ",
        "colab_type": "text"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcfuRb7AJs2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "  inp_sentence = [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "  enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "    \n",
        "  output = tf.expand_dims([VOCAB_SIZE_FR-2], axis=0)\n",
        "\n",
        "  for _ in range(MAX_LENGTH):\n",
        "    predictions = transformer(enc_input, output, False) # (1, seq_length, vocab_size_fr)\n",
        "\n",
        "    prediction = predictions[:,-1:,:]\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "\n",
        "    if predicted_id == VOCAB_SIZE_FR-1:\n",
        "      return tf.squeeze(output, axis=0)\n",
        "\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0)\n",
        "\n",
        "def translate(sentence):\n",
        "  output = evaluate(sentence).numpy()\n",
        "\n",
        "  predicted_sentence = tokenizer_fr.decode(\n",
        "      [i for i in output if i < VOCAB_SIZE_FR-2]\n",
        "  )\n",
        "\n",
        "  print(\"Input: {}\".format(sentence))\n",
        "  print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq1foAe6MCt3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5640a300-53d4-45f4-b8ad-148c9dd754e2"
      },
      "source": [
        "translate(\"The limits of my language are the limits of my world.\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: The limits of my language are the limits of my world.\n",
            "Predicted translation: Les limites de ma langue sont les limites de mon monde.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}